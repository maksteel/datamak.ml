["Department of Computer Science The University of Auckland New Zealand ABCD-SE: Automatic Bayesian Covariance Discovery with Stable Extrapolation Manish Ashoklal Kukreja October 2016 Supervisors: Dr. Yun Sing Koh Dr. Patricia Jean Riddle A dissertation submitted in partial fulfillment of the requirements of Master of Professional Studies in Data Science ","2 ","Abstract Machine learning algorithms are known for their predictive power — feed input to algorithm and you get a output with its likelihood or probability. Ensemble methods like random forest, artificial neural networks such as deep learning or statistical models, for example, support vector machines, often provide high accuracy. These algorithms first go through a process called “learning or modelling” the data. In learning phase, algorithms discover certain set of rules or parameters, usually referred as models . To most users, these learned models are seen as inscrutable , since many of them are not given training in machine learning or statistics. It is a big challenge, in artificial intelligence (AI) field, to learn (or select) a model that not only predicts the data well, but also is interpretable. As we try to make our models more descriptive, the chances of over-fitting increases. Bayesian non-parametric modelling, specifically Gaussian processes (GP) provide a way to fit a model that can be descriptive, interpretable and has more predictive power. GP can be learned easily on new data or model change-points in data via their covariance kernel function. This dissertation explore several key areas of an artificial intelligent system which performs data analysis through compositional kernel search called Automatic Bayesian Covariance Discovery (ABCD). We introduce a novel methods to stabilise the predictive performance of ABCD. From our experimental results, we show that our methods perform better extrapolation and can also explore a much wider model space in hopes of searching a richer interpretable structure. Thus, this dissertation is titled as Automatic Bayesian Covariance Discovery with Stable Extrapolation or ABCD-SE. i ","ii ","Acknowledgements I would like to acknowledge and thank all the people and organisations that helped me with my dissertation. First and foremost, I express my gratitude and respect to both of my supervisors Yun Sing Koh and Patricia Jean Riddle, for their constant guidance, pragmatic suggestions and useful feedback throughout this project. Their open and forward looking attitude towards research and reproducibility defined the success for this project. I would like to give a special thanks to James Robert Lloyd, who made his work of Automatic Bayesian Covariance Discovery algorithm as public, and thus I could extend his research, in the form of this dissertation. I wish to acknowledge the contribution of New Zealand eScience Infrastructure (NeSI) high-performance computing facilities to the results of this research. NZ’s national facilities are provided by the NZ eScience Infrastructure and funded jointly by NeSI’s collaborator institutions and through the Ministry of Business, Innovation &amp; Employment’s Research Infrastructure programme https://www.nesi.org.nz . Finally, I wish to thank my family, starting with my wife, Deepa Manish Kukreja, who kept me motivated towards the completion of this study and my Masters. My two very energetic kids Dron and Paarth who always cheered me up whenever I felt stressed throughout this work. Lastly, I thanks my parents for their blessings and support that I receive over the course of this work and beyond. iii ","iv ","Contents List of Tables ix List of Figures xi 1 Introduction 1 1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Problem statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.3 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.4 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.5 Overview of methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.6 Dissertation structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2 Literature Review 7 2.1 Machine learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.2 Supervised machine learning . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.2.1 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.2.2 Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2.3 Predicting output as probability . . . . . . . . . . . . . . . . . . . . 10 2.3 Probabilistic modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.3.1 Parametric versus non-parametric models . . . . . . . . . . . . . . . 11 2.3.2 Bayesian non-parametric modelling . . . . . . . . . . . . . . . . . . 12 2.4 Automatic pattern discovery . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.4.1 Multiple Kernel Learning . . . . . . . . . . . . . . . . . . . . . . . . 14 2.4.2 Compositional kernel search . . . . . . . . . . . . . . . . . . . . . . 17 2.5 Discussion of current research . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 3 Background 23 3.1 Modelling functions using Gaussian Processes . . . . . . . . . . . . . . . . 23 3.2 ABCD: A system for automatic data analysis . . . . . . . . . . . . . . . . 27 v ","vi Contents 3.2.1 Zero Mean Gaussian Process . . . . . . . . . . . . . . . . . . . . . . 28 3.2.2 Atoms of kernel language . . . . . . . . . . . . . . . . . . . . . . . . 28 3.2.3 Composition rules of kernel language . . . . . . . . . . . . . . . . . 29 3.2.4 Discovering composite kernel . . . . . . . . . . . . . . . . . . . . . . 31 3.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 4 Stabilising extrapolation in an automatic data-analysis system 35 4.1 Hypotheses development . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 4.1.1 Model search: greedy versus top-k . . . . . . . . . . . . . . . . . . . 36 4.1.2 Model score: BIC versus AIC . . . . . . . . . . . . . . . . . . . . . 37 4.1.3 Model evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 4.2 Proposed methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 4.2.1 Kernel search with top-k strategy . . . . . . . . . . . . . . . . . . . 40 4.2.2 Model evaluation with CV SoD method . . . . . . . . . . . . . . . . 42 4.3 Design of experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 4.3.1 Baseline Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . 43 4.3.2 Experiments with Hybrid SoD method . . . . . . . . . . . . . . . . 44 4.3.3 Experiments with CV SoD method . . . . . . . . . . . . . . . . . . 44 4.4 Error measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 4.5 Datasets used . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 4.6 Technical considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 4.6.1 Software considerations . . . . . . . . . . . . . . . . . . . . . . . . . 46 4.6.2 Hardware considerations . . . . . . . . . . . . . . . . . . . . . . . . 47 4.6.3 Source code availability . . . . . . . . . . . . . . . . . . . . . . . . . 47 4.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 5 Findings of experiments 49 5.1 Baseline experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 5.1.1 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 5.1.2 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 5.2 Experiments with Hybrid SoD method . . . . . . . . . . . . . . . . . . . . 50 5.2.1 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 5.3 Experiments with CV SoD method . . . . . . . . . . . . . . . . . . . . . . 52 5.3.1 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 5.4 Result analysis of experiments with Hybrid SoD and CV SoD methods . . 55 5.4.1 Analysis of variance (ANOVA) . . . . . . . . . . . . . . . . . . . . . 55 5.5 Experiments with no White Noise base kernel . . . . . . . . . . . . . . . . 60 5.5.1 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 5.5.2 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 ","Contents vii 5.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 6 Conclusion 67 6.1 Achievements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 6.2 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 6.3 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 6.4 Reflections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 Appendices 71 A Auto-generated report for 01-airline dataset (five base kernels) 73 B Auto-generated report for 01-airline dataset (four base kernels) 87 ","viii Contents ","List of Tables 2.1 Example expressions: Many common patterns of supervised learning can be captured using sums and products of one-dimensional base kernels . . . 18 4.1 List of experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 4.2 Dataset description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 4.3 NeSI’s AVX Node specifications . . . . . . . . . . . . . . . . . . . . . . . . 48 5.1 Baseline experiments results showing the best model using GNU Octave . . 50 5.2 Results showing best model using Matlab (Lloyd [14]) . . . . . . . . . . . . 50 5.3 Results of experiments with Hybrid SoD model evaluation method . . . . . 51 5.4 Results of experiments with CV SoD model evaluation method . . . . . . . 54 5.5 Prediction error SMSE of the best models discovered in experiments with four base kernels, greedy search and BIC scoring . . . . . . . . . . . . . . 61 5.6 Average number of models searched for each dataset, in all of the experi- ments, that searched three levels deep . . . . . . . . . . . . . . . . . . . . . 64 ix ","x LIST OF TABLES ","List of Figures 2.1 Part (a): Represents labelled training sample of coloured shapes, along with three unlabelled test samples [17]. Part (b): Represents training sample as an N × D design matrix. Each i th row represent feature vector x i along with class label y i ∈ { 0 , 1 } [17]. . . . 9 2.2 Part (a): Represents linear regression on a one-dimensional data [17]. Part (b): Represents polynomial regression (degree 2) on same data [17]. . 10 2.3 Left and third columns: base kernels k ( · , 0). Second and fourth columns: draws from a GP with each respective kernel. The x-axis has the same range on all plots [4]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.4 Examples of structures expressible by composite kernels. Left column and third columns: composite kernels k(; 0). Plots have same meaning as in Figure 2.3 [4]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.1 Bayesian linear regression, lines are random samples from prior probability distribution, the red shading represents the probability distribution. (a): Represents prior distribution on slope (or gradient) m [14]. (b): Represents the posterior distribution on slope m , after seeing 5 samples [14]. (c): Represents the posterior distribution on slope m , after 15 observations [14]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.2 Random samples from zero mean Gaussian process with different covari- ance functions. Lines are random samples drawn from Gaussian process distribution, the red shading represents the pointwise probability distribution. 26 3.3 Flow chart of Automatic Bayesian Covariance Discovery (ABCD) system [15]. 27 3.4 Five base kernels [29]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 3.5 Functions encoded by five base kernels [29]. . . . . . . . . . . . . . . . . . . 29 3.6 Examples of richer structures using compositional rules [29]. . . . . . . . . 30 3.7 Covariance function discovery using greedy search approach at depth level 1 33 3.8 Covariance function discovery using greedy search approach at depth level 2 33 xi ","xii LIST OF FIGURES 3.9 Covariance function discovery using greedy search approach at depth level 3 33 4.1 Covariance function discovery using top-three search approach at depth level 1 41 4.2 Covariance function discovery using top-three search approach at depth level 2 41 4.3 Covariance function discovery using top-three search approach at depth level 3 41 5.1 Scatter plots of baseline experiments . . . . . . . . . . . . . . . . . . . . . 53 5.2 Box plots for each level of independent variable . . . . . . . . . . . . . . . 56 5.3 Prediction error (SMSE) using Hybrid SoD model evaluation method . . . 57 5.4 Prediction error (SMSE) using CV SoD model evaluation method . . . . . 58 5.5 Interaction plots of search versus criterion versus evaluation factors . . . . 59 5.6 Prediction error (SMSE) of models constructed using four base kernels — SE , Per , Lin and C , in a greedy search strategy and selected based on highest BIC model score . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 5.7 Box plot of each model evaluation method . . . . . . . . . . . . . . . . . . 62 ","1 Introduction “We make our world significant by the courage of our questions and by the depth of our answers.” — Carl Sagan [ 24 ] In his best-selling book Cosmos , Sagan [ 24 ] expresses that if we wish to be worthy of attention, then we need to ask brave questions and find their deep and meaningful answers. We think, it is our ability to observe the world around us that deepens our knowledge and broadens our horizon. This curiosity has been fundamental to the development of the technologies that we cherish everyday. With the advent of every new technology we are confronted with new possibilities and challenges. Today, we live in a world that is surrounded by plethora of Artificial Intelligence (AI) from virtual assistants, email spam filters, music and movie recommendation services to fraud detection systems, these AI systems have become indispensable part of our everyday life. Even though AI has been around for long time. Only recently, we have started building AI systems that learn almost autonomously, without much human intervention or expertise, for instance, self-driving cars. However, human-level intelligence or learning is very much a distant dream. 1 ","2 Introduction 1.1 Motivation In human life, the idea that certain things “cause” other things, plays a vital role. In fact, we have designed our weather forecasting systems, financial markets and even statistical methods, around this philosophy. At a fundamental level, however, the particles and forces of modern physics, behave in predictable ways according to the laws of nature. It is kind of like, how a number after 55 is 56, and the number before it is 54, but 55 does not cause 54 or 56 — there is just a “pattern” traced out by those numbers. Modern scientific research points out, that at the very core of this universe, there are patterns between the events . With this understanding about the nature of the world, we wish to manifest an artificial intelligence that can discover these patterns in real-world and explain them in a human language. Automatic Statistician is one such project that aims to create an AI that can help us to make sense of our data. Lloyd et al. [ 15 ] created Automatic Bayesian Covariance Discovery (ABCD) as part of this project. ABCD is artificial intelligence that discovers the patterns, which describe the given set of observations or data and produces a report in English, like a human statistician would write. ABCD learns a Gaussian process model that can make precise predictions. The learnt model is interpretable due to its covariance kernel [ 4 ]. This simple and accurate interpretation of the model, is what makes this system more useful than other systems like artificial neural networks [ 18 ]. The ability to interpret the model allows users to make sense of vast amounts of data, without possessing expert knowledge about statistical or machine learning methods. Model Search and Model Evaluation are the two key phases in automatically learning a Gaussian process model. ABCD uses a greedy approach to model search [ 15 ]. In greedy search strategy, at each stage a highest scoring kernel function is expanded for further exploration [ 4 ]. Lloyd [ 14 ] argues that greedy approach is justified as this is how a human statistician develop their models, iteratively. We think since machines can explore a much larger model space efficiently compared to human statisticians, then why should we restrict the AI search only to a limited part of model space? Furthermore, at each stage of model search, several models are evaluated in parallel. The standard approach to Gaussian process regression is computationally expensive [ 23 ]. For that reason, ABCD uses an approximation to Gaussian process regression. This evaluation technique learns kernel parameters on a randomly sampled subset of data [ 14 ]. Chalupka, Williams, and Murray [ 3 ] concluded that subset of data method outperforms other approximation approaches, however, the choice of subset greatly influences the prediction accuracy of the model . Owing to the limitations discussed above and amount of experimentation carried out by Lloyd [ 14 ], we believe that further research is required in the areas of model search and ","1.2 Problem statement 3 evaluation of ABCD system. Our research proposal is to make model predictions stable while ensuring the high interpretability of the model. 1.2 Problem statement This dissertation extends the structure discovery research [ 4 , 14 , 15 ], to make stable and accurate extrapolation by maintaining high interpretability of the discovered structure. We are addressing this problem by proposing alternative methods to perform model search and evaluation in ABCD system. We intend to use a top-k search strategy for model space exploration and a novel cross-validated subset of data approximation to Gaussian process regression. These methods aim at improving the prediction capability of ABCD while maintaining the interpretability of its discovered models. Thus, we intend to provide stability to ABCD and refer our algorithm as Automatic Bayesian Covariance Discovery with Stable Extrapolation (ABCD-SE). 1.3 Objectives Through this work, we intend to improve the prediction performance of an existing automatic data-analysis system by enabling it to search a much wider model space and by retaining the interpretability of discovered models. In other words, we aim to address the challenge of carefully choosing the methods that both explore (search a best model) and exploit our knowledge about high interpretability of kernel functions in the Gaussian processes. In this dissertation, we have three major objectives: • To develop a search strategy which expands the model search space, in turn allowing ABCD to discover richer patterns in real-world data. • To analyse model evaluation using Akaike information criterion (AIC) that enables ABCD to consider the amount of training data. • To create a model approximation technique that utilises random selection subset of data method but improves the extrapolation of discovered models. 1.4 Contributions To meet our objectives we carefully calibrate exploration—exploitation tradeoff. We develop a model search strategy that selects top-three models at each stage of search, this allows ABCD to search at least 60% more model space as compared to greedy ","4 Introduction search. Using Kullback—Leibler information loss quantified by the AIC we aim to improve model selection process. Finally, we introduce a novel model evaluation method which combines the benefits of stable predicting performance provided by cross validation with fast approximation provided by subset of data parameter learning approach. Through meeting our objectives, we will make the following contributions to the field automatic machine learning: • Developing an effective model search procedure that exploits the power of high performance distributed computing called top-k search . • Analysing the improvement in model selection procedure by using AIC as the scoring criterion that incorporates the size of real-world data. • Introducing the novel idea of combining an efficient approximation to Gaussian process regression technique known as “Subset of Data” (SoD) with Cross Validation (CV) to provide stable predictions, we refer this method as CV SoD. 1.5 Overview of methodology To meet these objectives, we have explored the background for the methods we propose, by providing in-depth insights into the areas of supervised machine learning, Bayesian non-parametric modelling and compositional kernel search. We examined multiple kernel learning methods, multimodel inference principles and several techniques that approximate to Gaussian process regression. We have shown how our proposed methods operate and justified their applicability by analysing scientific research carried out in this field. We have conducted experiments on high performance computing platforms, additionally we describe the evaluation measures used to compare our results. To test our methods, we use the same, 13, univariate real-world data sets that were used by original ABCD study [ 14 ]. Moreover, we experiment with four multivariate real- numbered data sets to show the proposed methods suitability on high dimensional data. In order to assess the influence of random sampling, we repeated all of our experiments five times. Finally, we have shown results for testing our techniques against original approaches of greedy model search, Bayesian information criterion scoring and random selected subset of data model evaluation on all 17 datasets. We have shown that our methods achieve better and stable extrapolation while ensuring high interpretability of the models. ","1.6 Dissertation structure 5 1.6 Dissertation structure In this introduction chapter, we have provided some background and context for the work we have done. Our motivation comes from the realisation that AI can be a useful tool for helping us make sense of large amounts of data, where it is not always possible to have good knowledge of the statistical or machine learning methods. We have stated the contributions we intend to make through this work. We have briefly described our methodology for testing our proposed methods to show that they are suitable for the purposes outlined. In Chapter 2 , we explore supervised machine learning and need for Bayesian non- parametric modelling and contrast how it is different from parametric modelling. We examine the literature by discussing automatic pattern discovery, the important considera- tions when performing analysis in this area, compared to traditional data analysis. We detail how some recent compositional kernel search algorithms have been designed to meet these constraints. In Chapter 3 , we explore the background to this work in greater depth. We detail the work and concepts that closely relate to the methods we propose in this dissertation. Specifically, we explain how Gaussian processes are used for modelling functions, how ABCD exploits this property of Gaussian processes through a greedy search strategy, how it utilises compositional covariance discovery algorithm, discuss search operators and its Bayesian Information Criterion based scoring approach to model selection and explain the underlying concept of Zero Mean Gaussian Process. In Chapter 4 , we describe the hypotheses that our dissertation is testing, we explain the steps of our proposed methods, top-k search and cross validated subset of data. We explain how they are implemented. We then describe each set of experiments undertaken, with parameters selected and clarifying the intended purposes. We clearly define the evaluation measures used for evaluating the quality and performance of our techniques. We describe the datasets we have used in our experiments, including the source of our real-world datasets. We outline the technical platform used to run our experiments. In Chapter 5 , we tabulate the results that we have received from conducting the experiments described in the prior chapter. We provide analysis of the results for each set of experiments. We provide an overview of what we have learnt from our experiments. We relate the results seen back to the contributions, as well as any areas in which our proposed methods failed to meet our intended aims. We discuss future work, both in terms of refinement and development, of the methods we have proposed. In Chapter 6 , we conclude the dissertation by summarising the major contributions of our work, relating them back to our experiments, points out future directions and add some final reflections and remarks. ","6 Introduction ","2 Literature Review In this chapter, we delve into our research area of Bayesian non-parametric learning using compositional kernel search. We first explore supervised learning and its popular methods, then we discuss the need for probabilistic modelling. We further discuss limitations of parametric modelling and benefits of Bayesian non-parametric modelling. Furthermore, we discuss a framework for performing automated pattern discovery using compositional kernel search by comparing with several approaches developed earlier for multiple kernel learning. Finally, we conclude by discussing the research opportunities in recent methods and share the focus of this dissertation. 2.1 Machine learning Machine learning (ML) is the branch of science that makes computers perform certain tasks like image, text or speech recognition, playing games such as ‘Go’ or even driving cars, without being manually programmed for that task. In machine learning, the computer learns a task by analysing given data and then performs it, as a human would do. ML can be broadly categorized into two groups - predictive and descriptive. In predictive (also called supervised learning), the objective is to learn the relationship between the input x and output y , when a labelled set D of N observations or input and output pairs is given. Sometimes, D is referred as ‘training sample’, ‘training data’ or 7 ","8 Literature Review simply ‘given data’. D = { ( x i , y i ) } N i =1 (2.1) In training sample, each input x i ∈ R is a D -dimensional vector, that can represent simple information such as no. of passengers in a flight or weight and height of a person or complex information such as a molecular structure, image, graph and so on. They are usually referred as attributes, features or co-variates. Similarly the output y can be a real number (e.g. amount of carbon emission) or a categorical variable (e.g. male or female). Furthermore, depending upon value of y , the learning problem can be referred to as classification (when y is categorical) or regression (when y is real number). The other broad category in machine learning is descriptive (also called unsupervised learning), in this approach, the objective is to discover the interesting patterns or knowledge in the given data. Hence, training sample contains the inputs that are not associated with any output, generally known as unlabelled inputs, D = { x i } N i =1 (2.2) This is usually not a well-defined problem, as there is no direction given as to what pattern to look for. As a result, there are no apparent error measures that can be used, as in case of supervised learning where predicted value of y i can be compared with observed value of y i for a given x i . There is, however, a third category in machine learning which is called reinforcement learning. This type of learning is beneficial for learning the behaviour or action when a reward or penalty signal is received. Our research problem falls in the realm of supervised machine learning, so we continue our investigation further into that area. 2.2 Supervised machine learning We continue to explore machine learning by discussing its most widely used form i.e. supervised learning. As mentioned earlier depending upon the type of output target the supervised learning task can be considered as classification or regression. Following subsections discuss them in detail. 2.2.1 Classification The objective of a classification task is to learn the mapping or relationship between inputs x and outputs y , where y ∈ { 0 , 1 , 2 , .., C } with C being the number of classes. It is called binary classification when C = 2 , and usually in this case y ∈ { 0 , 1 } . It is called multi-class classification when C &gt; 2. When class labels are not mutually exclusive, then it is called multi-label classification or multiple output model. In this thesis, whenever we ","2.2 Supervised machine learning 9 Figure 2.1: Part (a): Represents labelled training sample of coloured shapes, along with three unlabelled test samples [ 17 ]. Part (b): Represents training sample as an N × D design matrix. Each i th row represent feature vector x i along with class label y i ∈ { 0 , 1 } [ 17 ]. use the term “classification” we are referring it to a multi-class classification with single output. Murphy [ 17 ] suggests, a way to formalize the classification problem, by viewing it as a function approximation task. Murphy assumes, y = f ( x ) (2.3) for some unknown function f , and the objective of the learner is to estimate the function f for a given labelled training sample. Then make predictions using, ˆ y = ˆ f ( x ) (2.4) hat symbol denotes estimate. Murphy also explains that prime objective of classifier is to make predictions on novel inputs x * , i.e. the input the classifier is not trained on, it is also called test sample. Prediction on test sample is known as generalization of learner. A binary classification example is shown in Figure 2.1 . In this example, the task is to learn which shapes belong to class label 1 and which belong to class label 0. And make prediction about the class or classify three unlabelled shapes they are blue crescent, yellow circular ring and blue arrow. A fair guess the learner could do is to classify blue crescent as y = 1 , as all blue shapes appear in class 1. The yellow circular ring and blue arrow are somewhat difficult to classify as some circular rings (blue rings) are assigned y = 1 and other y = 0 (red rings) also, yellow shapes appear on both sides as well. Similar situation is faced for blue arrow classification all blue coloured shapes are assigned y = 1 whereas a red arrow is assigned y = 0. This requires little more than just assigning the class to unseen data or need for some way to show confidence about the generalization beyond training set. ","10 Literature Review Figure 2.2: Part (a): Represents linear regression on a one-dimensional data [ 17 ]. Part (b): Represents polynomial regression (degree 2) on same data [ 17 ]. 2.2.2 Regression This section explores regression. In general regression is just like classification except the output target y is a continuous or real number. Murphy [ 17 ] mentions that linear regression is similar to line fitting between x i , and y i or formally as, y i = D X d =0 ( β d × x i,d ) i = { 1 ..N } (2.5) here, N is number of observations in training data, D is dimensions or features of input x , β 0 is offset of the line with { x i, 0 = 1 } N i =1 and { β 1 , β 2 ..β D } are the coefficients x d or slope of line. Figure 2.2 shows a simple example. In this example a one-dimensional real-valued input x i ∈ R , and response or output variable also real-valued i.e. y ∈ R . Two models are fitted to the data in 2.2 part (a) a straight line and in 2.2 part (b) a quadratic function. Murphy argues that several problems can arise in this learning approach such as outliers in observations, non-linear or non-smooth responses, or high-dimensional inputs. There have been several methods developed to tackle such problems such as measuring Cook’s distance [ 16 ] to detect outliers, use of quadratic function or generalized linear models for non-linear or non-smooth responses, feature selection to deal with high-dimensional inputs. In both methods — classification and regression, there is a need to provide some sort of confidence about the prediction that is been done by these models. 2.2.3 Predicting output as probability In order to handle ambiguity, such as yellow ring case in classification or use of linear versus quadratic in regression, it is best-suited to return a probability rather than a class or a real number for output y . Rasmussen and Williams [ 23 ] describe the process of generalization, on test inputs, involves inherent uncertainty and it is natural to make ","2.3 Probabilistic modelling 11 predictions that depicts this intuition about uncertainties. The probability distribution of output y , given test input x * and training data D is represented as p ( y | x * , D ). The probabilistic output ˆ y is computed by maximizing ˆ f ( x ) or loss function as shown below, ˆ y = ˆ f ( x ) = C argmax c =0 p ( y = c | x * , D ) (2.6) corresponds to predicting most probable class in a probabilistic classification model, often referred as mode of the probability distribution p ( y | x * , D ). It is known as MAP (maximum a posteriori) estimate, whereas, in regression finding the value of ˆ β that maximizing the log-likelihood of the ˆ f ( x ) is known as MLE (maximum likelihood estimator) and is given by, ˆ β mle ⊆ argmax β ∈ Θ ˆ l ( β | x 1 , ..., x n ) (2.7) ˆ l ( β | x ) = 1 n n X i =1 ln f ( x i | β ) (2.8) Murphy [ 17 ] describes certain real world applications where probabilistic predictions are used such as IBM’s Watson system to provide an answer when asked a question based on its confidence in the answer and Google’s SmartASS (ad selection system) which based on user’s search history and other features predicts the probability of user clicking an ad, this probability is better known as the CTR (click through rate). The list is growing enormously due to its benefits. To continue our exploration, we discuss further about two kinds of probabilistic models. 2.3 Probabilistic modelling There are several approaches to define probabilistic models of the form p ( y | x ), the most important distinction among them is the whether a model has fixed number of parameters or do they grow with respect to the amount of the training set. 2.3.1 Parametric versus non-parametric models The model with fixed number of parameters in it, is called a parametric model. The model in which number of parameters increase as the amount of training set, is called a non-parametric model. Both these models have their own advantages and limitations over each other. Parametric models are often faster to build, however, they make very strong assumptions about the probability distributions of the training set. On the other hand, non-parametric models are very flexible about the nature of data distributions, on the down side they are often computationally demanding for big training sets. ","12 Literature Review Orbanz and Teh [ 19 ] provide some examples of each type, like fitting a Gaussian (a normal distribution) or a mixture of a fixed number of Gaussians by maximum likelihood is one way of doing parametric modelling. Parzen window estimator is given as example for non-parametric modelling, this estimator fits a Gaussian for each observation (i.e. it uses one mean parameter for every observation). Another example for non-parametric modelling is Support Vector Machine (SVM) with a Gaussian kernel, because the model complexity (i.e. number of parameter in Gaussian kernel) increases with more observations. The performance of non-parametric methods has been impressive over the years and therefore they gained lot of popularity in non-Bayesian (classical) statistics. On the contrary, Orbanz and Teh [ 19 ] argues that, the theoretical results of these models are harder to prove than that of parametric models. Consequently, Bayesian non-parametric methods emerged. Their results could be proved theoretically, as they provide a Bayesian framework for model selection and adaptation using non-parametric models. Next section discusses Bayesian non-parametric models with popular examples. 2.3.2 Bayesian non-parametric modelling Orbanz and Teh [ 19 ] describes Bayesian non-parametric models as being non-trivial cases. A Bayesian model defines a prior and infers posterior distributions on one-dimensional parameter space. In a non-parametric approach, however, the dimensions of parameter space should adapt with the increase in training set. Wilson [ 27 ] describes this change in parameter space as model’s information capacity that grows with data. Wilson mentions that model scaling is a desirable property, because the model must explain (or express) more information as it has been trained on more data. Thus, a Bayesian non-parametric model is the one that, contains a Bayesian model on an infinite-dimensional (or finite but unbounded dimensional) parameter space and can be analysed on a finite training data in such a way that it only uses a finite group of the available parameters to explain the training data. Additionally, the parameter space is usually represented by random functions and measures or probability distributions on infinite-dimensional random objects. These functions or distributions are called as stochastic processes. Dirichlet processes, Gaussian processes and beta processes are some of the examples of such stochastic processes. In practice, the Bayesian non-parametric models are named after the stochastic processes they are comprised of. If we are to represent a full function with a Bayesian non-parametric model, then we might need an infinite number of parameters. Chapter 3 shows that learning and inference of Bayesian non-parametric models is possible with finite computing resources. Wilson [ 27 ] provides several reasons why Bayesian non-parametric modelling is best suited for pattern discovery and extrapolation. First being the information capacity of such models, ","2.4 Automatic pattern discovery 13 that scales with size of training dataset. Secondly, these models reflect that we do not know the exact form of the function that created the training dataset. Third reason talks about the prior information that these models can express, which allows them to make impressive generalizations. And fourth reason emphasizes on the expressiveness of such models i.e. models become more expressive when amount of available data is more. In spite of so many advantages, Bayesian non-parametric models, and specifically, Gaussian processes have mainly been used for smoothing and interpolation on smaller datasets [ 27 ]. As these models can express rich structure of large scale datasets, they posses the potential for developing intelligent systems, that can perform modelling automatically and generalize better on future datasets. We will now look at the literature that shows how to exploit this property of rich expressiveness of Bayesian non-parametric models using Gaussian Processes. 2.4 Automatic pattern discovery The emergence of Big Data and High Performance Computing has motivated ‘machine learning’ and development of data-driven model as opposed to explicit and fully structured models. Several techniques have been developed, and perfected over the past two decades. These tools can automatically learn interesting mappings. Few notable examples include tools like Hidden Markov Models, Neural Networks, Bayesian Network Inference, Support Vector Machines and Decision Trees [ 11 ]. Neural networks were introduced in 1980’s and have come a long way. Krizhevsky, Sutskever, and Hinton [ 12 ] highlighted the advancement in neural network techniques for large datasets like deep neural networks. The deep neural architecture provided remarkable results on non-linear regression problems. Wilson et al. [ 28 ] described the reason behind the popularity of neural networks is their ability to automatically discover the patterns in data and represent these patterns through non-linear functions. Wilson et al. [ 28 ] argues that this ability to learn patterns automatically (or expressiveness) comes at the cost of interpretability. On the other hand, Gaussian processes research in machine learning, grew out of neural networks research [ 18 ] and mid 1990’s marked the beginning of kernel or Bayesian non-parametric modelling in machine learning. Kernel methods such as Gaussian processes are flexible and more interpretable than neural networks, for e.g. the prior distributions over functions can be controlled in a covariance kernel (or kernel function) via properties such as smoothness, periodicity and so on. In fact, Neal [ 18 ] shows, as the number of hidden units in a Bayesian neural network approach infinity, it becomes a Bayesian non-parametric Gaussian processes with a neural network covariance kernel . Thus, Gaussian processes are viewed as non-parametric kernel ","14 Literature Review machines that can fit any data, by automatically calibrating their complexity, these models can be interpreted by their covariance kernel and provide a probabilistic framework for learning kernel hyper-parameters. Gaussian processes are often used as flexible statistical tools, where a human-expert manually discovers structure in data and then embeds (or hard codes) that structure into a kernel. More specifically, Gaussian processes are used with squared exponential or Mat´ ern covariance kernels. Even though these kernels represent overall structure, that structure, however, is often the simplified version of the data. If we put it in other way, using default kernels like Gaussian (or squared exponential) or Mat´ ern the model provides the global structure of the dataset and fail to provide local structure (or has simplified the local structure). It is not that Gaussian processes are incapable of representing local information, but they are limited by choice of kernels and their combinations [ 27 ]. As most of the kernel design/ selection is performed by human experts and they could only look at handful of combinations. The resultant Gaussian processes have to face this problem. There has been a significant amount of research in the area of multiple kernel learning, next section is exploring that aspect. 2.4.1 Multiple Kernel Learning Over the years researchers have come up with several methods to perform multiple kernel learning (MKL). In these methods researchers combine several kernel functions together so that rich expressiveness along with flexibility is achieved. Gonen and Alpaydn [ 8 ] provided a taxonomy of multiple kernel learning algorithms. Gonen and Alpaydn [ 8 ] divided MKL algorithms based on six key properties like learning methods, functional form, training methods, target function, base learner and computational complexity. We will look at some of those properties that are relevant to our dissertation. Learning methods These categories determine which learning methods MKL algorithms use to combine the kernel functions. Categories are given as below: 1. Fixed rules : In this category the kernel functions are combined without any parame- ters (e.g. summation or product of kernels). 2. Heuristic approaches : The kernels are combined using a parametrized function. The parameters for this combination function are found by comparing some heuristic measure obtained separately from each kernel. ","2.4 Automatic pattern discovery 15 3. Optimization approaches : In this approach the kernel combination function is parametrized as well. The parameters for this combination function are found by solving an optimization problem. 4. Bayesian approaches : In this method, kernel combination parameters are interpreted as random variables. They are learnt by putting a prior distribution over these parameters and performing Bayesian inference. 5. Boosting approaches : This is done by adding a new kernel in the combination until the model performance stops improving. This technique is inspired from ensemble and boosting methods. Functional forms These groups represent how the kernels are combined. They have been grouped under three categories: 1. Linear combination : This method of combining kernel is the most popular one. It has two sub-categories: non-weighted sum (or mean) of kernels as combined kernel and weighted sum. 2. Non-linear combination : These methods combine kernels with non-linear functions, for example, multiplication, power, and exponentiation. 3. Data-dependent combination : These methods assign specific kernel weights for each observation in training dataset. In this way, they try to identify local distributions in the data and learn appropriate combination of kernels for each region of data. Target functions In order to select the parameters of kernel combination function, different MKL algorithms try to optimize the target function differently. They were grouped into three basic categories: 1. Similarity-based functions : A kernel combination function that maximizes the similar- ity metric, its parameters are chosen as the best. The similarity metric is calculated between the combined kernel matrix and an optimum kernel matrix calculated from the training data. Similarity measures can be calculated using Kullback-Leibler (KL) divergence, kernel alignment, Euclidean distance or any other similarity measure. 2. Structural risk functions : In this framework, the MKL algorithms try to minimize the complexity of model (or sum of regularization term) and system performance (represented by an error term). Usually, structural risk functions use the l 1 -norm, the ","16 Literature Review l 2 -norm, or a mixed-norm on the kernel weights, that are integrated in regularization term, to pick the model parameters. 3. Bayesian functions : A kernel function is created from candidate kernels using a Bayesian formulation and its quality is measured. Generally, likelihood or the posterior is used as the target function. Model parameters that give the maximum likelihood estimate or the maximum a posteriori estimate are selected. Training methods MKL algorithms are grouped under two categories based on their training methods for determining the parameters of kernel combination function and parameters 1 of combined- kernel base learner: 1. One-step methods : In this group, both the parameters of kernel combination function and the parameters of the base learner (or kernel-based learner) are computed in a single pass. These parameters can be learnt sequentially or simultaneously. When learning parameters sequentially, the combination function parameters are calculated first, and then using the combined kernel, a base learner is trained. Whereas, in simultaneous approach, both sets of parameters are learned together. 2. Two-step methods : It is an iterative approach consisting of two steps to compute both sets of parameters. Each iteration starts with updating the parameters of kernel combination function, by keeping the base learner parameters fixed. In second step, the parameters of base learner are updated, and the combination function parameters are kept fixed. These two steps are repeated until convergence. Base Learner Owing to their empirical success support vector machines (SVM) and support vector regression (SVR) are the most commonly used base learners. SVM and SVR can be easily applied in two-step methods and also can be easily used in optimization problems in one-step training methods using simultaneous approach. Other popular base-learners, used in MKL algorithms, include kernel ridge regression (KRR), Kernel Fisher discriminant analysis (KFDA) and regularized kernel discriminant analysis (RKDA). The most common base learner used in Bayesian approaches is Gaussian process (GP). There are new inference algorithms proposed for modifying the probabilistic models in order to learn both sets of parameters. 1 Sometimes parameters of combined-kernel are referred as hyper-parameters ","2.4 Automatic pattern discovery 17 2.4.2 Compositional kernel search Typically, multiple kernel learning involves hand crafting combinations of Gaussian kernels for specialized applications [ 8 ] [ 4 ]. For example, modelling low dimensional structure in high dimensional data. These learning methods provide rich expressiveness but are not intended for automatic pattern discovery and extrapolation [ 27 ]. As described earlier Gaussian processes research progressed in machine learning from neural network research. However, Gaussian processes have a rich history in statistics, they are often used as distributions over functions, with properties controlled by a kernel [ 23 ]. In spite of providing many benefits like flexibility, interpretability, consistency, simple exact learning and inference procedures, suitability for kernel learning, and impressive empirical performances [ 23 ], Gaussian processes as kernel machines are still not in widespread use. This is mostly due to the fact, choosing the structural form of the kernel in non- parametric regression requires considerable expertise and trail and error [ 4 ] [ 27 ]. As a result, fields that currently rely heavily on expertise of statisticians, machine learning researchers and data scientists are compelled to apply Gaussian processes as smoothers and interpolation on their datasets [ 15 ]. In order to make Gaussian processes more applicable, Duvenaud et al. [ 4 ] proposed a method (explained in following sub-sections) that could automatically combine (or compose) kernels of a Gaussian processes using simple rules such as addition and multiplication. By looking at the problem of kernel learning as that of a structure discovery, the choices of kernel can be automated. Thus, transforming the opaque art of kernel engineering to transparent science of automated kernel construction. Representing structure through kernels Gaussian process (GP) models use a kernel to define the covariance or similarity between any two function (or output) values: Cov ( y, y 0 ) = k ( x, x 0 ). The kernel represents the possible structures under the GP prior. This structure in turn determines the generalization properties of the model. By using simple rules such as summation and multiplication, kernel families can be composed to express variety of prior over functions. Commonly used kernels families include the squared exponential ( SE ), periodic ( PER ), linear ( LIN ), and rational quadratic ( RQ ) (see Figure 2.3 and Chapter 3 ). The kernels which define a valid covariance function are known as positive semi-definite kernels. These positive semi-definite kernels when added or multiplied together results in a combined kernel, that is also a semi-definite kernel. By exploiting this property, richly structured and interpretable kernels can be created from well understood base kernels. All of the base kernels used in this method are one-dimensional; for multidimensional inputs, kernels are constructed by adding and ","18 Literature Review multiplying kernels over single dimensions. These individual dimensions are represented using sub-scripts, e.g. SE 2 represents an SE kernel over the second dimension of x . Next, we look at the operators for representing the structure. Some examples of common patterns of supervised learning can be seen as sum and products of base kernels in Table 2.1 . Summation Rule Superposition of individual functions is modelled by summing those kernels. It is quite possible that each of those individual kernels are representing a different structure in data. For instance functions f 1 , f 2 are draw from independent GP priors, f 1 ∼ GP (1 , k 1 ) , f 2 ∼ GP (2 , k 2 ). Then summation of f 1 &amp; f 2 is given by f := f 1 + f 2 ∼ GP (1 + 2 , k 1 + k 2 ). Sums of kernels can express superposition of different GPs, possibly operating at different scales, for a time-series model. Sometimes, summing kernels gives additive structure over different dimensions in a multi-dimensional input space, this is similar to generalized additive models (GAM). These two types of structures are illustrated in rows 2 and 4 of Figure 2.4 , respectively. Table 2.1: Example expressions: Many common patterns of supervised learning can be captured using sums and products of one-dimensional base kernels Bayesian linear regression LIN Bayesian polynomial regression LIN × LIN × . . . Generalized Fourier decomposition PER + PER + . . . Generalized additive models D ∑ d =1 SE d Automatic relevance determination D Q d =1 SE d Linear trend with local deviations LIN + SE Linearly growing amplitude LIN × SE Multiplication Rule To account for interactions between different input dimensions, two kernels are multiplied. For instance, the multiplicative kernel SE 1 × SE 3 represents a smoothly varying function of dimensions 1 and 3 which is not constrained to be additive, in a multidimensional data. In univariate data, multiplying a kernel by SE gives a way of converting global structure to local structure. For example, PER corresponds to globally periodic structure, whereas PER × SE corresponds to locally periodic structure, as shown in row 1 of Figure 2.4 . Many architectures for learning complex functions, such as backpropogation convolu- tional networks LeCun et al. [ 13 ] and sum-product networks Poon and Domingos [ 21 ], include units which compute AND-like and OR-like operations. A composite kernel consists ","2.4 Automatic pattern discovery 19 Figure 2.3: Left and third columns: base kernels k ( · , 0). Second and fourth columns: draws from a GP with each respective kernel. The x-axis has the same range on all plots [ 4 ]. of sums can be viewed as an OR-like operation, since two points are considered similar if either kernel has a high value in covariance matrix. And composite kernels consists of products can be viewed as an AND-like operation: two points are considered similar only if both kernels have high values in covariance matrix. Searching over structures A wide variety of kernel structures can be created compositionally by adding and multi- plying a small number of base kernels. In particular, Duvenaud et al. [ 4 ] considers the four base kernels: SE, PER, LIN and RQ . Their search procedure begins by applying all base kernels to all input dimensions. Then following search operators are applied over the set of expressions: 1. Any sub-expression S can be replaced with S + B , where B is any base kernel family. 2. Any sub-expression S can be replaced with S × B , where B is any base kernel family. 3. Any base kernel B may be replaced with any other base kernel family B 0 . These operators generate all possible algebraic expressions. When these operators are applied only on base kernels with + and × rules, a context-free grammar (CFG) is obtained, which generates the set of algebraic expressions. ","20 Literature Review Figure 2.4: Examples of structures expressible by composite kernels. Left column and third columns: composite kernels k(; 0). Plots have same meaning as in Figure 2.3 [ 4 ]. This algorithm uses a greedy search approach to search over this space i.e. at each stage, the highest scoring kernel is chosen and it is expanded by applying all possible operators. These search operators are inspired from the strategies human experts use to construct composite kernels. For example, • One can look for structure, e.g. periodicity, in the residuals of a model, and then extend the model to capture that structure. This corresponds to applying rule 1 . • One can start with structure, e.g. linearity, which is assumed to hold globally, but find that it only holds locally. This corresponds to applying rule 2 to obtain the structure shown in rows 1 and 3 of Figure 2.4 . • One can add features incrementally, analogous to algorithms like boosting, back- ","2.5 Discussion of current research 21 fitting, or forward selection. This corresponds to applying rules 1 or 2 to dimensions not yet included in the model. Scoring kernel families Choosing kernel structures requires a criterion for evaluating structures. We choose marginal likelihood as our criterion, since it balances the fit and complexity of a model [ 22 ]. Conditioned on kernel parameters, the marginal likelihood of a GP can be computed analytically. However, to evaluate a kernel family we must integrate over kernel parameters. We approximate this intractable integral with the Bayesian information criterion (BIC) [ 25 ] after first optimizing to find the maximum-likelihood kernel parameters. Unfortunately, optimizing over parameters is not a convex optimization problem, and the space can have many local optima. For example, in data with periodic structure, integer multiples of the true period (i.e. harmonics) are often local optima. To alleviate this difficulty, we take advantage of our search procedure to provide reasonable initializations: all of the parameters which were part of the previous kernel are initialized to their previous values. All parameters are then optimized using conjugate gradients, randomly restarting the newly introduced parameters. This procedure is not guaranteed to find the global optimum, but it implements the commonly used heuristic of iteratively modelling residuals. 2.5 Discussion of current research As noted by Plate [ 20 ], one of the main advantages of additive models such as GAM is their interpretability. Plate also notes that by allowing high-order interactions as well as low- order interactions, one can trade off interpretability with predictive accuracy. By exploring structure space with the help of context-free grammar and simple search operators that combine kernels in sums and products form. Then evaluating those kernels using BIC, provides an automated framework to construct kernels that are interpretable, flexible and have good predictive accuracy [ 4 ]. Since search operations and rules are applied to the similarity functions (or covariance kernels) rather than the regression functions (output functions) themselves, compositions of even a few base kernels are able to capture complex relationships in data which do not have a simple parametric form [ 4 ]. Duvenaud et al. [ 4 ] work was extended in Lloyd et al. [ 15 ]. This extension included several changes that resulted in a system called Automated Bayesian Covariance Discovery (ABCD). ABCD added new base kernels like White noise ( WN ), constant ( CONST ) and removed rational quadratic ( RQ ). New rules were added like changepoint CP ( · , · ) and changewindow CW ( · , · ). To prove the interpretability of discovered models a natural language processing (NLP) component was added to ABCD system. NLP components ","22 Literature Review translated GP model into human readable report in English language with figures decom- posing individual kernel in composite kernel. In our view, ABCD system uses a combination of several approaches within each of the key properties, presented by Gonen and Alpaydn [ 8 ] for multiple kernel learning algorithms. In particular, • Learning methods for kernel combination function falls into fixed rules, Bayesian and boosting approaches. • Functional form represents linear (non-weighted sums) and non-linear (products) combination. • Target functions are optimized using Bayesian inference or likelihood and combined kernel function is evaluated using BIC that is similar to structural risk functions. • Training methods used in ABCD system is one-step sequential method. • Base learner is Gaussian processes. As expressed by Duvenaud et al. [ 4 ] that their compositional kernel search does not guarantee to find global optimum for hyper-parameters. During experimentation with un-penalized marginal likelihood, overfitting was observed in ABCD system [ 14 ]. These scenarios provide us opportunity to explore other methods of hyper-parameter learning and model evaluation criteria. As a result, this dissertation focuses on the need for cross-validated hyper-parameter learning, top-k search and model evaluation using Akaike information criterion (AIC) [ 1 ]. 2.6 Summary Machine learning provides exciting opportunities to develop data-driven intelligent systems. The dependency on human-expert is significantly influencing the choice of models created by such intelligent systems. There is a need for an automated framework that can discover patterns from given dataset and learn basic components that are interpretable and overall model is flexible enough to generalize on unseen data. To fully realise the dream of automated predictive systems, designing such a framework is important. Machine learning community is trying to solve this problem by proposing several methods. ABCD is one such system that attempts to bridge the gap between accuracy and interpretability with flexible and automated modelling. ","3 Background In this section we will introduce Bayesian non parametric modelling using Gaussian processes and then discuss the concept of covariance kernel with its importance in Gaussian process models. In the second half of this section we will describe an existing automatic system for data analysis, its essential elements and procedures. Towards the end, we will discuss the limitations of existing automatic system and briefly propose the changes we wish to experiment in this dissertation. 3.1 Modelling functions using Gaussian Processes Gaussian processes have been introduced in many ways by several authors in literature. The introduction that is most relevant to this dissertation is given by Rasmussen and Williams [ 23 ]. We would like to start from linear regression and slowly progress towards Gaussian processes. This section is adapted from Lloyd [ 14 ]. In a simple form of Bayesian linear regression model, we would have a training data D that contains input and output pairs as, D = { ( x i , y i ) ∈ R 2 : i = 1 , .., N } (3.1) A simple probabilistic linear regression model for D could be represented as, 23 ","24 Background y i ∼ mx i + i (3.2) m ∼ N (0 , 1) (3.3) i iid ∼ N (0 , σ 2 ) (3.4) Equations 3.2 , 3.3 and 3.4 represents our belief that output y i is linearly related to input x i and a line passing through origin depicts that relationship. Our beliefs would update once we see the data D , as per probability theory we would find updates as, y i | D ∼ N x j ∑ i x i y i ∑ i x 2 i + σ 2 , x 2 j σ 2 ∑ i x 2 i + σ 2 + σ 2 j 6∈ { 1 , .., N } (3.5) m | D ∼ N ∑ i x i y i ∑ i x 2 i + σ 2 , σ 2 ∑ i x 2 i + σ 2 (3.6) Figure 3.1 shows the updates of prior belief for m (Equation 3.3 ) into posterior belief (Equation 3.6 ). As more and more data is observed (or seen) the posterior belief on parameter (or slope) m gets more concentrated. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 (a) Prior belief 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1 (b) Posterior after 5 samples 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1 (c) Posterior after 15 observations Figure 3.1: Bayesian linear regression, lines are random samples from prior probability distribution, the red shading represents the probability distribution. (a): Represents prior distribution on slope (or gradient) m [ 14 ]. (b): Represents the posterior distribution on slope m , after seeing 5 samples [ 14 ]. (c): Represents the posterior distribution on slope m , after 15 observations [ 14 ]. In order to represent linear relationship between y = ( y i ) and x = ( x i ) as a noisy relationship, we can rewrite these Equations as ( 3.7 , 3.8 ). y ∼ N (0 , x 2 i + σ 2 ) (3.7) Cov( y i , y j ) = x i x j 2200 i 6 = j (3.8) Now, if we assume the prior distribution of y to be a multivariate Gaussian distribution (Equation 3.9 ), we get, y ∼ N (0 , K ) where K ij = x i x j + δ ij σ 2 (3.9) ","3.1 Modelling functions using Gaussian Processes 25 In Equation 3.9 , δ ij is the Kronecker delta. When we represent covariance K ij as a function of input pairs k ( x i , x j ) we get, K ij = k ( x i , x j ) = x i x j + δ x i x j σ 2 (3.10) Equation 3.10 is true for all datasets of any finite size. We can also define a probability distribution over y when x is infinite ( x = R ). In a collection of random variables, when any finite subset have a joint Gaussian distribution, that collection is referred as a Gaussian process. To specify a Gaussian process, we need to specify the mean and covariance of any finite subset of random variables. Mean can be represented by a mean function μ : X → R and covariance can be represented by a covariance function or kernel k : X 2 → R , where X is the input space x , if we substitute mean and covariance functions in Equations 3.7 and 3.8 we get, y i ∼ N ( μ ( x i ) , k ( x i , x j )) (3.11) E ( y i ) = μ ( x i ) (3.12) Cov( y i , y j ) = k ( x i , x j ) (3.13) or ( y i : i = 1 , ..., N ) ∼ N (( μ ( x i ) : i = 1 , ..., N ) , ( k ( x i , x j ) : i, j = 1 , ..., N )) (3.14) If we replace y i = f ( x i ) in Equation 3.14 , we would define a probability distribution over functions f . However, matrix K = k ( x i , x j ) must be a positive semi-definite, only then we can define a valid probability distribution. This brings us to another definition of Gaussian processes, they are the probability distributions over functions, for any finite subset of random variables that have a joint Gaussian distribution and are denoted as, f ∼ gp ( μ, k ) (3.15) What is a covariance kernel? The kernel k directly specifies the covariance (or similarity) between a pair of random function values (or output points) at a pair of input points: k ( x i , x j ) = Cov ( f ( x i ) , f ( x j )). A covariance kernel therefore encodes inductive biases - what sorts of solution functions we expect. By choosing a covariance function, we choose whether the solution functions are periodic, smooth, linear, polynomial and so on. In other words, we hardly need to modify our algorithm to radically change the model - we just change the covariance function. We have so far used the terms kernel, covariance kernel and covariance function ","26 Background interchangeably. In general, a kernel is a function that maps any pair of inputs into R . The covariance function of a Gaussian process is an example of a kernel. For k ( x i , x j ) to be a valid covariance function of a Gaussian process, any matrix K with elements K ij = k ( x i , x j ) must be positive semi-definite; this requirement follows since the covariance matrix in a Gaussian distribution must be positive semi-definite. Encoding inductive biases Let us now explore the properties of Gaussian process i.e. probability distribution over functions by changing μ and k . The mean function μ provides pointwise mean of the distribution over functions. The kernel function k controls the shape of the sample function that can be drawn from this distribution over functions. It is interesting to note, the changes in the patterns of random samples, drawn from Gaussian process that have different kernel functions. Figure 3.2 , for instance, shows random sampled functions drawn from a zero mean Gaussian process with linear, smooth and periodic kernel functions. (a) Linear functions (b) Smooth slow varying functions (c) Smooth periodic functions Figure 3.2: Random samples from zero mean Gaussian process with different covariance functions. Lines are random samples drawn from Gaussian process distribution, the red shading represents the pointwise probability distribution. Kernel function used in Figure 3.2 (a) was same as Equation 3.10 with noise variance equal to zero, as shown below, k ( x i , x j ) = x i x j (3.16) Kernel function used in Figure 3.2 (b) was of form k ( x i , x j ) = exp( - α | x i - x j | 2 ) where α is a constant (3.17) In this distribution, when distance between x i and x j is small the covariance (and correla- tion) between f ( x i ) and f ( x j ) will be approximately equal to 1. As the distance | x i - x j | increases the correlation decreases monotonically to zero. This decreasing correlation determines the smoothness characteristic of the random sampled function in Figure 3.2 (b). Lastly, the kernel function used in Figure 3.2 (c) was of the form, k ( x i , x j ) = exp - 2 sin 2 ( π ( x i - x j ) /p ) l 2 (3.18) ","3.2 ABCD: A system for automatic data analysis 27 where p and l are constants. In this distribution when | x i - x j | is an integer multiple of p , the correlation is equal to 1, this results in exact periodic characteristic of random sampled function in Figure 3.2 (c). In summary, a Gaussian process is a collection of random variables, within which a finite set of variables have a joint Gaussian distribution. A Gaussian process can be completely specified by its mean and kernel functions. A mean function represents the estimation of input function, whereas kernel function represents similarity between input pairs. By choosing a different kernel function, we can model different pattern of training data. We can even combine several kernel functions together to model complex but interpretable patterns of data. This property allows us to create different probabilistic models by specifying different kernels or combinations of them. However, choosing a kernel function requires extensive human expertise and is often time consuming as it follows trail and error approach. Owing to these reasons, the use of Gaussian processes is very limited. In the next section, we discuss how ABCD system overcomes these limitations by automatically learning different combinations of kernel functions that best describe the given dataset. 3.2 ABCD: A system for automatic data analysis Automatic Bayesian Covariance Discovery (ABCD) system is designed to automatically learn a Gaussian process model with zero mean and a composite kernel. The GP model is searched in a open-ended model space using greedy search approach. Figure 3.3 shows a high level flow chart of the system. Ellipses represent input to and output from various procedures. Rounded rectangles represent the procedures. This system is divided into two stages - model search and report generation. Data Search Language of models Evaluation Model Prediction Description Checking Report Figure 3.3: Flow chart of Automatic Bayesian Covariance Discovery (ABCD) system [ 15 ]. In model construction/ search stage, a open-ended model space is searched, in attempt ","28 Background to best describe or discover patterns from training data using a language of models. The search procedure is iterative and at each level a searched model is evaluated using Bayesian Information Criterion (BIC). The model with optimum BIC score is chosen for next iteration (i.e. search follows a greedy approach). In report generation stage, the best selected model is used to perform extrapolation or prediction on test data, a Natural Language Processing (NLP) component interprets its composite kernel and converts it into English language statements and a model checking procedure performs model criticism. By combining all these information a report is generated in English language that describes the model and its performance. This dissertation focusses on model search and model evaluation procedures. We therefore paint the components that are not part of this dissertation in grey colour, such as model checking, description and report generation (Figure 3.3 ). 3.2.1 Zero Mean Gaussian Process Discovering patterns in given dataset automatically using Gaussian processes would require a language of models, a model search procedure and a model evaluation framework. Since, a Gaussian process is completely determined by its mean and kernel function, { f ( x ) | a ∼ gp ( a × μ ( x i ) , k ( x i , x j )) } where a ∼ N (0 , 1) (3.19) If mean is marginalised out, we get a zero mean Gaussian process as shown below, f ( x ) ∼ gp (0 , μ ( x i ) μ ( x j ) + k ( x i , x j )) (3.20) In a zero mean Gaussian process, the structure of the kernel describes high level characteristics of the unknown function, f , which eventually determines how the model generalizes or extrapolates to new data. Using a zero mean Gaussian processes, we can therefore define a language for Gaussian process regression models as ‘language of kernels’. 3.2.2 Atoms of kernel language 0 0 0 0 0 Squared exp. ( SE ) Periodic ( Per ) Linear ( Lin ) Constant ( C ) White noise ( WN ) Figure 3.4: Five base kernels [ 29 ]. This language has a set of five base kernels, also referred as the basic elements. Each ","3.2 ABCD: A system for automatic data analysis 29 base kernel function captures a different function property. When we create a composite kernel by combining these base kernels we still get a valid kernel i.e. it satisfies requirement of positive semi-definite [ 15 ]. These base kernels are shown in Figure 3.4 , they are squared exponential ( SE ), periodic ( Per ), linear ( Lin ), constant ( C ) and the white noise ( WN ). Base kernels are defined as follows, these equations are applicable to scalar-valued inputs: SE ( x i , x j ) = σ 2 exp - ( x i - x j ) 2 2 ‘ 2 (3.21) Per ( x i , x j ) = σ 2 exp   cos 2 π ( x i - x j ) p ‘ 2   - I 0 ( 1 ‘ 2 ) exp ( 1 ‘ 2 ) - I 0 ( 1 ‘ 2 ) (3.22) Lin ( x i , x j ) = σ 2 ( x i - ‘ )( x j - ‘ ) (3.23) C ( x i , x j ) = σ 2 (3.24) WN ( x i , x j ) = σ 2 δ x i ,x j (3.25) where l and p are parameters 1 of the kernel functions, δ x i ,x j is the Kronecker delta function, I 0 is the modified Bessel function of the first kind of order zero. These base kernels - squared exponential ( SE ), periodic ( Per ), linear ( Lin ), constant ( C ) and the white noise ( WN ) encode for smooth functions, periodic functions, linear functions, constant functions, and uncorrelated noise respectively. These encodings are shown in Figure 3.5 . Smooth functions Periodic functions Linear functions Constant functions Gaussian noise Figure 3.5: Functions encoded by five base kernels [ 29 ]. 3.2.3 Composition rules of kernel language There are two main composition rules of kernel language, they are addition and multiplica- tion. Summing two kernels together can be viewed as OR’ing them, since two input points are considered similar if either kernel has a high value in covariance matrix (Equation 3.26 ). ( k 1 + k 2 )( x, x 0 ) = k 1 ( x, x 0 ) + k 2 ( x, x 0 ) (3.26) Similarly, product of two kernels can be viewed as AND’ing them together, since two 1 Also known as hyperparameters . ","30 Background input points are considered similar only if both kernels have high values in covariance matrix (Equation 3.30 ). ( k 1 × k 2 )( x, x 0 ) = k 1 ( x, x 0 ) × k 2 ( x, x 0 ) (3.27) When combining base kernels using sum and product operations, yields kernels that encode richer structures, such as approximate periodicity ( SE × Per ) or smooth functions with linear trends ( SE + Lin ), more examples can be seen in Figure 3.6 . Lin × Lin SE × Per quadratic functions locally periodic Lin + Per SE + Per periodic plus linear trend periodic plus smooth trend Figure 3.6: Examples of richer structures using compositional rules [ 29 ]. There are two more advanced operators included in language to model changepoints in time-series data. They are changepoint CP ( · , · ) and changewindow CW ( · , · ). Changepoints are defined by addition and multiplication with sigmoidal functions: CP ( k 1 , k 2 ) = k 1 × σ + k 2 × ¯ σ (3.28) where σ = σ ( x i ) σ ( x j ) and ¯ σ = (1 - σ ( x i ))(1 - σ ( x j )). Similarly, by replacing σ ( x i ) with a product of two sigmoids changewindow CW ( · , · ) operator is defined. This dissertation does not use CP ( · , · ) and CW ( · , · ) operators and focusses on the kernels that are composed using +, × and replacement search operators as represented below: S → S + B (3.29) S → S × B (3.30) B → B 0 (3.31) where S represents any kernel sub-expression and B and B 0 are any base kernel. These search operators represent addition (Equation 3.29 ), multiplication (Equation 3.30 ) and replacement (Equation 3.31 ). ","3.2 ABCD: A system for automatic data analysis 31 3.2.4 Discovering composite kernel As per our discussion so far, we can say that kernel is the heart of a Gaussian process. It has the potential to greatly change a Gaussian process model, without requiring to change inference or learning procedures. We have also discussed that using a small number of base kernels compositionally i.e. by adding or multiplying we can construct a variety of sophisticated structures. In particular, ABCD system uses five base kernel families: SE , Per , Lin , C and WN . The compositional framework described in Section 2.4.2 and Duvenaud et al. [ 4 ] has been extended and adapted by ABCD system. In ABCD system, the base kernels are different and search operators include changepoint and changewindow along with summation and multiplication. Algorithm 1 Automatic Bayesian Covariance Discovery 1: kernels ← base kernels: { SE , Per , Lin , C , WN } 2: operators ← { +, ×} 3: criterion ← BIC 4: best Model ← Perform Kernel Search ( kernels , operators , criterion ) 5: ypred ← Make Predictions ( best Model , xtest ) At a high level, to discover a composite kernel and measure its performance ABCD system performs three steps as formalised in Algorithm 1 . In the first step, necessary variables, like base kernels, operators, criterion used for model evaluation are all initialised. The kernel search is performed next, this step discovers the composite kernel using Gaussian process regression that best describes the given training dataset. Algorithm 2 formalises the steps involved in kernel search. Finally, ABCD system performs prediction on test data to verify how well the discovered model generalises on unseen data. A working example Consider a training data D = { ( x i , y i ) } N i =1 , where x i is a vector of size d representing number of input dimensions or features. The search procedure begins by applying all base kernels to all input dimensions (see Figure 3.7 and steps 2 to 4 in Algorithm 2 ). Once initial set of GP models are proposed, search proceeds to model fitting and evaluation (steps 5 to 8 in Algorithm 2 ). The kernel parameters, θ , of each of these proposed GP models are optimised using conjugate gradient descent [ 23 ]. Then, each proposed model, M , is evaluated using marginal likelihood (or model evidence) p ( D | M ), that is computed analytically [ 23 ] as shown below, p ( D | M ) = Z p ( D | θ opt , M ) p ( θ opt | M ) dθ opt (3.32) ","32 Background where θ opt is optimised kernel parameters. After evaluation, the marginal likelihood, p ( D | M ), is penalised for the optimised kernel parameters, θ opt , using Bayesian Information Criterion (BIC) [ 25 ] as shown below, BIC( M ) = - 2 log p ( D | M ) - p log N (3.33) where p is the number of kernel parameters. The value of BIC ( M ) becomes the score of model, M (step 7 in Algorithm 2 ). The model with highest score (i.e. lowest BIC ( M ) value) is selected as best model for current depth level (the best model is shown in a green box in Figure 3.7 and step 9 in Algorithm 2 ). If the desired depth is not reached, then the search continues a level deeper (steps 10 to 14 in Algorithm 2 ). Algorithm 2 Automatic Bayesian Covariance Discovery: Kernel Search 1: procedure Perform Kernel Search ( kernels , operators , criterion ) 2: { M } ← Create GP Models ( kernels ) 3: { M } ← Random Restart Covariance Functions ( M ) 4: { M } ← Remove Duplicate Models ( M ) 5: for model in { M } do 6: likelihoods[model] ← Evaluate Model ( model , xtrain , ytrain ) 7: scores[model] ← Calculate Score ( likelihoods[model] , criterion ) 8: end for 9: best model ← Highest Score ( scores , criterion ) 10: if depth ≤ desired depth and Is Model Score Improved ( scores[best model] , threshold ) then 11: depth ← depth + 1 12: { M } ← Expand Kernels ( M[best model] , kernels , operators ) 13: go to 3 14: end if 15: return M[best model] 16: end procedure In next level, the best GP model is expanded by applying all possible search operators (Equations 3.29 , 3.30 and 3.31 ) and a new set of proposed models are created (see Figure 3.8 ). Again, each new proposed model is evaluated, scored and the model with highest score is selected as best model for current depth level (the best model is shown in a green box in Figure 3.8 ). Search procedure is stopped, if there is no improvement in model scores beyond certain threshold or desired search depth is reached. This search approach is referred as greedy search and is visualised in Figure 3.9 , green boxes show models that scored highest at a particular level. After reaching to desired depth in search tree, the best model of that depth level is returned to main procedure (step 15 in Algorithm 2 ). The best model is then used to make predictions about the data (step 5 in Algorithm 1 ). ","3.2 ABCD: A system for automatic data analysis 33 Start SE Per Lin C WN Figure 3.7: Covariance function discovery using greedy search approach at depth level 1 Start SE Per Per + SE . . . Per × SE . . . Per × WN Lin C WN Figure 3.8: Covariance function discovery using greedy search approach at depth level 2 Start SE Per Per + SE . . . Per × SE ( Per × SE ) + SE . . . ( Per × SE ) + Lin . . . . . . . . . . . . . . . Per × WN Lin C WN Figure 3.9: Covariance function discovery using greedy search approach at depth level 3 ","34 Background 3.3 Summary In this section we described how Gaussian Process regression model can describe a different structure of data, by changing the covariance (or kernel) function, without changing the learning method. We also discussed how ABCD system learns or discovers the composite kernel automatically in a greedy search approach. Usually, researchers construct a composite kernel in such iterative way, therefore this simple strategy is employed in ABCD system. Additionally, a previous study has demonstrated empirical performance using such search strategy [ 9 ]. There are, however, certain limitations to the model search approach and model scoring criteria. Lloyd [ 14 ] suggested that use of BIC to approximate model evidence is not contextually justified due to difficulties in finding tractable marginal likelihood. Lloyd [ 14 ] also, argued that optimising over parameters is not a convex optimisation problem, and space could have local optima. Although hot start and random restarts are used, they do not guarantee global optima. Lloyd [ 14 ] mentioned that we would not want to optimise all parameters fully since we have to look at many models and it will be computationally expensive to fully optimise all parameters on full training data for each model. This dissertation proposes several techniques in attempt to overcome these limitations, they are discussed in detail in section 4 . ","4 Stabilising extrapolation in an automatic data-analysis system This dissertation extends the Gaussian process structure search research initiated by an automatic data-analysis system called Automatic Bayesian Covariance Discovery (ABCD). We experiment, with a much broader model search strategy like top-k, using the Akaike Information Criterion (AIC) [ 1 ] as model evidence instead of BIC and/ or with cross-validated Subset of Data (SoD) hyper-parameter optimisation technique for model approximation. These methods aim at improving prediction performance of ABCD, consequently we named our algorithm as Automatic Bayesian Covariance Discovery with Stable Extrapolation (ABCD-SE). In this section, we discuss the hypotheses that were developed as part of this disser- tation. Following the hypotheses development, we present novel methods proposed in this dissertation. We then provide the design of experiments, that were performed to validate those hypotheses using proposed methods. Furthermore, we present the evaluation measure used for comparing the results of experiments. Additionally, we provide detailed information about the datasets used in the experiments. At the end of this section, we describe the software and hardware related considerations. 35 ","36 Stabilising extrapolation in an automatic data-analysis system 4.1 Hypotheses development In this section, we present the hypotheses that are tested in this dissertation and we also discuss the rationale behind the developing them. Overall, we have three hypotheses based on our focus areas i.e. model search approach, model scoring and model evaluation. 4.1.1 Model search: greedy versus top-k In greedy search approach, the model search begins by proposing all base kernels, these models are evaluated and a model that has a highest score is chosen as best model. In the next level of search, the best model is expanded using kernel search operators, these expanded models are again evaluated and a new best model is selected based on highest score. Section 3.2.4 describes greedy approach in detail and Figure 3.9 shows a typical example of greedy search. Greedy search approach looks very similar to step-wise forward selection approach used in linear regression [ 10 ]. Step-wise forward selection approach starts by proposing a model with no parameters. Then add one parameter at a time, if the model evidence gets better the algorithm adds new parameters in model and so on. Step-wise forward selection is often criticised in literature [ 5 ] for providing poor parameter estimates. These criticisms may not be directly applicable to non-parametric Bayesian Gaussian process regression that ABCD system uses. Since the original thesis [ 14 ] did not experiment with other approaches. An alternative search strategy to greedy search would be to select top-k models at each search level. In top-k search approach the algorithm chooses top-k models that had highest scores among all proposed models and continues the tree expansion for all selected models in the next level. Analysis of reports generated by Lloyd et al. [ 15 ], reveal that around 80% to 90% of variability of data is explained by top-three components of composite kernel. We therefore, set k = 3 and search for the top-three models at each level. Model search starts with five base kernels, top-k ( k = 3) strategy would be able to explore 60% more model-space and possibly discover richer structures in data, as compared to greedy search. We describe covariance discovery using top-k search in detail in Section 4.2.1 . Compared to top-k strategy, model-space explored by greedy search is very limited. Consequently, richer structures or patterns may not be discovered by greedy search at all. We formalise it in Hypothesis 1 . Hypothesis 1 Composing a kernel greedily, results in a most interpretable Gaussian process regression model that provides stable extrapolation. ","4.1 Hypotheses development 37 4.1.2 Model score: BIC versus AIC Model selection is one of the biggest challenges in automatic machine learning [ 6 , 7 ]. In literature, the problem of comparing several models, in order to best describe the given training data and assuming that, it would generalise well on unseen test data, is often referred as “Multimodel Inference” [ 2 ]. Burnham and Anderson [ 2 ] argue that practitioners often try to improve the inference or model selection by establishing a selection criterion such as Bayesian Information Criterion (BIC) or Akaike information criterion (AIC). They describe the central issue in multimodel inference is, to first, establish the philosophy about data analysis, whether there exists a true model or that all models are just approximations? Only then, we should find a suitable criterion. They presented three guiding principles for model-based inference: • Multiple working hypotheses is establishing the philosophy that all proposed models are approximations to the full reality and that there is no true model. • Simplicity and Parsimony of models is the amount of information (or truth) a particular model is representing, whether the model is extracting simple information (discovering large structures/pattern) or complex information (many local patterns). Additionally, what size of model is good enough to extract such information, does a parsimony model represent high information that is well generalisable or can large complex models be justified? This is a bias versus variance trade-off. Inference under models with too few parameters can be biased, while with models having too many parameters may treat “noise” in data as signal. • Strength of evidence is quantifying the uncertainty (or information loss) of a fitted model. This quantitative information is derivable from likelihood estimates. By abiding these principles, Burnham and Anderson [ 2 ] state that a good model selection criterion should reduce to “a number” for each fitted model. This number, quantifies the uncertainty, about each proposed model being the best model. A good criterion must also be based on the philosophy about the data are finite and “noisy”. Two well-known approaches that satisfy these requirements are Bayesian model selection based on Bayes factors and information-theoretic selection based on Kullback-Leibler (K-L) information loss. BIC represents Bayes factor approach and AIC represents the K-L information loss approach [ 2 ]. ABCD system uses likelihood-based strength-of-evidence model comparison by em- ploying BIC (Equation 3.33 ) as measure of model evidence. Model selection can also be performed using criterion as AIC with a correction ( AIC c ) for finite sample sizes (Equation ","38 Stabilising extrapolation in an automatic data-analysis system 4.1 )[ 26 ]. AIC c ( M ) = - 2 log p ( D | M ) + 2 p + 2 p ( p + 1) ( n - p - 1) (4.1) where, p is number of kernel parameters in trained model M and n total number of observations in training data D . Burnham and Anderson [ 2 ] argue that BIC results in parsimonious models compared to AIC 1 . It is not guaranteed, however, that this parsimony model would often represent lower information loss. This lack of certainty provides us with a research opportunity, accordingly we formulate Hypothesis 2 . Hypothesis 2 Models selected with highest BIC score, results in a most interpretable Gaussian process regression model that provides stable extrapolation. 4.1.3 Model evaluation The standard implementation of Gaussian Process Regression (GPR) requires O ( n 2 ) space and O ( n 3 ) time for a data set of n examples [ 23 ] and involves three phases: • hyper-parameter learning : In this phase the hyper-parameters of kernel function are learned. For example, by maximizing the log marginal likelihood. Usually, this phase is computationally expensive. • training : In this phase Gaussian process (GP) model is fitted to given training data using the learnt (or optimised) hyper-parameters. Usually this phase involves computing the Cholesky decomposition of K + σ 2 n I on training data. • testing : In this phase, computations are carried out on unseen test data. The computations can be significant if the test set is very large. ABCD system carries out first two phases — hyper-parameter learning and training as part of “model evaluation”. After discovering the best covariance composition using greedy search strategy, ABCD system carries out testing phase, the system uses the best GP model to perform predictions (or extrapolation). ABCD searches the open-ended model space by fitting large number of models, in parallel, to discover best covariance kernel composition. Given a large dataset or a system that needs to perform multimodel inference, GPR using standard approach would require extensive computational power. Thus, in these scenarios an approximation technique is required. Chalupka, Williams, and Murray [ 3 ] compared several popular approximation methods with standard implementation and concluded that Subset of Data technique outperforms all of the other approximation 1 We use AIC and AIC c interchangeably in this dissertation ","4.1 Hypotheses development 39 techniques. Subset of Data method simply ignores some or most of the data i.e. a full GP prediction method is applied to a subset of size m &lt; n . Accordingly, if standard approach to GPR is followed in model evaluation in a system like ABCD, it would require considerable amount of computational resources. For that reason, ABCD system uses a approximation method to perform GPR. ABCD system uses a two-step approximation method — first, learning of hyper-parameters on randomly sampled (or selected) subset of data and then learning hyper-parameters on full data. In this dissertation, we refer to this two-step method as Hybrid SoD method. Algorithm 3 formalises this approach. Algorithm 3 Automatic Bayesian Covariance Discovery: Model Evaluation 1: procedure Evaluate Model ( model , xtrain , ytrain ) 2: xsub ← Random Sample ( xtrain ) 3: ysub ← Random Sample ( ytrain ) 4: 5: for iter in { 1 to iters } do 6: theta opt ← Minimise ( theta opt , xsub , ysub ) 7: end for 8: 9: for full iter in { 1 to full iters } do 10: theta opt ← Minimise ( theta opt , xtrain , ytrain ) 11: end for 12: 13: likelihood ← GP Fit ( theta opt , xtrain , ytrain ) 14: return likelihood 15: end procedure Chalupka, Williams, and Murray [ 3 ] stated that subset selection in SoD method could influence the model performance. ABCD system randomly chooses the subset of size m &lt; n , since original thesis [ 14 ] performed all experiments only once, we do not have evidence that this random selection of subset gives stable performance or not. However, by analysing Hybrid SoD method we believe that extrapolation results are highly dependent on the seed chosen for the random number generator used for sampling of subset. To address this limitation, we developed a novel approximation method, that makes an informed choice when selecting a subset of data. The subset selection decision is based on 10-fold cross-validated mean absolute predicted error. We call this method as Cross-Validated SoD (CV SoD), it is described in detail in Section 4.2.2 . Based on these arguments we form Hypothesis 3 Hypothesis 3 Models evaluated with Hybrid SoD method, does not always result in a most interpretable Gaussian process regression model that provides stable extrapolation. ","40 Stabilising extrapolation in an automatic data-analysis system 4.2 Proposed methods In this section, we introduce the methods that we developed to test the alternative hypotheses. We would first describe the model search with top-k strategy while contrasting it with greedy search approach. We then move on to Cross Validated Subset of Data (CV SoD) a novel model approximation technique to Gaussian process regression. 4.2.1 Kernel search with top-k strategy We, now, discuss how covariance function or composite kernel is discovered using top-three search method. Like greedy approach, top-three method starts by proposing models with only base kernels. These proposed models are evaluated, scored and ranked. Top-three ranked models are selected for next level of search. Figure 4.1 shows an example of top-three models selected at first level. A proposed model containing only periodic ( Per ) base kernel function has scored highest and is selected as first rank model (shown in green colour), model containing only linear ( Lin ) base kernel function is at second place (shown in red colour) and model containing only constant ( C ) base kernel function is ranked third (shown in blue colour). Algorithm 4 ABCD: Kernel Search using top-k strategy 1: procedure Perform Kernel Search ( kernels , operators , criterion ) 2: { M } ← Create GP Models ( kernels ) 3: { M } ← Random Restart Covariance Functions ( M ) 4: { M } ← Remove Duplicate Models ( M ) 5: for model in { M } do 6: likelihoods[model] ← Evaluate Model ( model , xtrain , ytrain ) 7: scores[model] ← Calculate Score ( likelihoods[model] , criterion ) 8: end for 9: top k models ← Get Top K Models ( scores , criterion ) 10: if depth ≤ desired depth and Is Model Score Improved ( scores[top k models] , threshold ) then 11: depth ← depth + 1 12: { M } ← Expand Kernels ( M[top k models] , kernels , operators ) 13: go to 3 14: end if 15: return M[top k models] 16: end procedure In next search level, each of these selected models are expanded by applying all search operators described in Equations 3.29 , 3.30 and 3.31 . This expansion results in a new set of proposed models. These proposed models are evaluated, scored and ranked. Among those scored models, top-three are selected for next level of search. Figure 4.2 shows the top-three models selected at second level, they are represented as first ranked model in ","4.2 Proposed methods 41 green colour, second ranked model in red colour and third ranked model in blue colour. In Figure 4.2 , models proposed after expansion of constant ( C ) base kernel would not longer be part of further search, as none of its proposed model made into top-three ranks. However, search would be continued in two sub-branches of linear ( Lin ) base kernel. Start WN C Lin Per SE Figure 4.1: Covariance function discovery using top-three search approach at depth level 1 Start SE Per Lin C WN Per + SE . . . Per × WN Lin + SE . . . Lin × Lin . . . C + SE . . . Figure 4.2: Covariance function discovery using top-three search approach at depth level 2 Start SE Per Lin C WN Per + SE . . . Lin + SE . . . Lin × Lin . . . . . . ( Lin + SE ) + SE . . . ( Lin + SE ) + Lin . . . ( Lin × Lin ) + SE . . . Figure 4.3: Covariance function discovery using top-three search approach at depth level 3 ","42 Stabilising extrapolation in an automatic data-analysis system In search level three, we further expand the selected models by applying search operators. Then evaluate, score and rank them based on the scores. Figure 4.3 shows how search might proceed in level three. In this example we can see that top-three search has discovered a structure, which would not have been possible if greedy search was used. In second level, Model with kernel function as ( Lin + SE ) had a lower score than that of a model with kernel function as ( Per + SE ). Whereas, in level three, all proposed models obtained after the expansion of ( Per + SE ), scored less than those models that were generated by expanding ( Lin + SE ). 4.2.2 Model evaluation with CV SoD method In CV SoD method, full training data is divided ten equal parts (or folds), we then perform 10 iterations over it. In each iteration, nine folds are chosen as train-set and a remaining fold is chosen as validate-set. Then, in next iteration other nine folds are chosen as train-set and a fold that was part of train-set earlier is now chosen as validate-set. In this way, each fold is once chosen as validate-set. Inside each iteration, a random sample is chosen from train-set called as subset of data (SoD). We then optimise hyper-parameters on SoD, fit a GP model on SoD and perform prediction on validate-set. After 10 iterations we get 10 predictions and the GP model which results in minimum mean absolute prediction error, its hyper-parameter are further optimised on full training data, a final GP model is fitted on full training data and likelihood is returned. We formalise this procedure in Algorithm 5 . 4.3 Design of experiments This dissertation extends the research carried out by Lloyd et al. [ 15 ] by performing experiments in the areas of model search, model scoring and model evaluation. We divide our study into three parts, they are baseline experiment (Section 4.3.1 ), assessments with Hybrid SoD method (Section 4.3.2 ) and assessments with CV SoD method (Section 4.3.3 ). In each set of experiments we opt for a particular model searching method and a model scoring criteria, this is based on a 2 × 2 × 2 factorial or balanced design. Table 4.1 gives a list of experiments performed in this dissertation. In each of our experiments the given dataset was divided into two parts training data (90%) and test data (10%). We discovered the best model on training data and made predictions on test data. We used five base kernels as shown in Figure 3.4 and three search operators as described in Equations 3.29 , 3.30 and 3.31 . Specific details about experimental set-up are mentioned in their respective subsections. ","4.3 Design of experiments 43 Algorithm 5 ABCD: Model Evaluation using CV SoD method 1: procedure Evaluate Model ( model , xtrain , ytrain ) 2: k ← 10 3: xtrain cv ← Make CV Partition ( xtrain , k ) 4: ytrain cv ← Make CV Partition ( ytrain , k ) 5: 6: for fold in { 1 to k } do 7: xsub ← Random Sample ( xtrain cv[!fold] ) 8: ysub ← Random Sample ( ytrain cv[!fold] ) 9: xvalid ← xtrain cv[fold] 10: yvalid ← ytrain cv[fold] 11: 12: for iter in { 1 to iters } do 13: theta opt[fold] ← Minimise ( theta opt[fold] , xsub , ysub ) 14: end for 15: 16: ypred ← GP Fit ( theta opt[fold] , xsub , ysub , xvalid ) 17: MAE[fold] ← Mean Of Absolute Differences ( ypred - yvalid ) 18: end for 19: 20: theta opt ← theta opt[ Index Of Min (MAE)] 21: 22: for full iter in { 1 to full iters } do 23: theta opt ← Minimise ( theta opt , xtrain , ytrain ) 24: end for 25: 26: likelihood ← GP Fit ( theta opt , xtrain , ytrain ) 27: return likelihood 28: end procedure 4.3.1 Baseline Experiments The purpose of these experiments were to make sure that using GNU Octave instead of Matlab 2 produces similar results. In baseline experiments, we performed model search and prediction without any change in ABCD system i.e. we used the Algorithm 2 approach to discover covariance kernel function greedily. We did not experiment with any of the system parameters with different values, as the purpose was to establish a baseline, we kept same values as set by original thesis [ 14 ], for example, we used the same random seeds, kept iters = 250, full iters = 10, subset size = 250 and desired depth = 10. We performed these experiments on four out of 13 univariate datasets used in original thesis [ 14 ], they were 01-airline, 06-internet, 07-call-centre and 08-radio. 2 Matlab was used by Lloyd et al. [ 15 ] for their experiments ","44 Stabilising extrapolation in an automatic data-analysis system Table 4.1: List of experiments Experiment Model Search Model Score Baseline Greedy BIC With Hybrid SoD method Greedy BIC Greedy AIC Top-k BIC Top-k AIC With CV - SoD method Greedy BIC Greedy AIC Top-k BIC Top-k AIC 4.3.2 Experiments with Hybrid SoD method The purpose of these experiments was to assess the influence of randomly selection subset of data in Hybrid SoD model evaluation method. We performed four experiments, as seen in Table 4.1 . In first experiment we used greedy search strategy with BIC as model selection criterion. In second, we used greedy search with AIC as model selection criterion. In third and fourth, we used top-k search approach with BIC and AIC as model selection criterion, respectively. These experiments were performed to validate all the hypotheses mentioned Section 4.1 . We performed each experiment five times on 13 univariate datasets and four multi- variate datasets, each time we used a different random seed. As the purpose of these experiments was to assess the influence of model evaluation on extrapolation and due to the shared allocation of hardware infrastructure, we restricted model search to three levels deep (i.e. desired depth = 3). Moreover, we did not experiment with system parameters and kept them as what they were used in original thesis [ 14 ], for example, we kept iters = 250, full iters = 10 and subset size = 250. 4.3.3 Experiments with CV SoD method The purpose of these experiments was to assess the influence of cross-validated selection subset of data using novel CV SoD model evaluation method. We performed four experi- ments, as seen in Table 4.1 . In first experiment we used greedy search strategy with BIC as model selection criterion. In second, we used greedy search with AIC as model selection criterion. In third and fourth, we used top-k search approach with BIC and AIC as model selection criterion, respectively. These experiments were performed to validate all the ","4.4 Error measures 45 hypotheses mentioned Section 4.1 . We performed each experiment five times on 13 univariate datasets and four multi- variate datasets, each time we used a different random seed. As the purpose of these experiments was to assess the influence of model evaluation on extrapolation and due to the shared allocation of hardware infrastructure, we restricted model search to three levels deep (i.e. desired depth = 3). Moreover, we did not experiment with system parameters and kept them as what they were used in original thesis [ 14 ], for example, we kept iters = 250, full iters = 10 and subset size = 250. 4.4 Error measures The best Gaussian process models, that were constructed by performing the experiments described in Sections 4.3.2 and 4.3.3 , were used for predicting the test data (10% of original dataset). We measured the accuracy of each model’s prediction on test data using the Mean Squared Error (MSE) (Equation 4.2 ) and then standardised it, so as to compare the model performance of different methods on different datasets. Rasmussen and Williams [ 23 ] defined Standardised Mean Squared Error (SMSE) in their book and we formulated it in Equation 4.3 . MSE( M ) = μ ( [ y test - y pred ] 2 ) (4.2) SMSE( M ) = MSE( M ) Var( y test ) + ( μ ( y test )) 2 (4.3) where, y test is the vector of output target values of test data, y pred is a vector of predicted output values, predicted by model M , μ is statistical mean, Var is the sample variance. 4.5 Datasets used We used 17 real world datasets. We performed experiments on 13 datasets that were used originally by Lloyd et al. [ 15 ] (refer datasets 1 to 13 in Table 4.2 ), these datasets are available in their repository. The remaining four datasets were borrowed from various sources, their description is given below. 14-quebec-xl: Quebec births dataset Abstract : Number of daily birth in Quebec, from Jan 1, 1977 to Dec 31, 1990. Available at http://research.ics.aalto.fi/eiml/datasets.shtml ","46 Stabilising extrapolation in an automatic data-analysis system 15-concrete: Concrete Compressive Strength Data Set Abstract : Concrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. Available at https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength . 16-parkinsons: Parkinsons Telemonitoring Data Set Abstract : Oxford Parkinson’s Disease Telemonitoring Dataset. Available at https: //archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring . 17-winewhite: Wine Quality Data Set Abstract : Dataset includes white vinho verde wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests. Available at https: //archive.ics.uci.edu/ml/datasets/Wine+Quality 4.6 Technical considerations In this section, we describe the existing software tools and libraries used to stabilise extrapolation in an automatic data-analysis system and hardware platforms on which the experiments were run. 4.6.1 Software considerations We used Automatic Bayesian Covariance Discovery (ABCD) system as our automatic data-analysis system. The implementation of ABCD, we used was originally created as part of the automatic statistician ( http://www.automaticstatistician.com ) project and is available at https://github.com/jamesrobertlloyd/gpss-research . This implementation of ABCD system uses a modified version Gaussian process regression and classification toolbox version 3.1, this toolbox is popularly known as Gaussian Processes for Machine Learning (GPML) toolbox, available at http://www. gaussianprocess.org/gpml/code . We ran ABCD system using Python version 2.7.9 with numpy (1.9.2) and scipy (0.15.1) libraries. We used GNU Octave version 4.0.0 to interface with GPML tool- box. We performed our experiments on machines running Red Hat Enterprise Server release 6.3 (Santiago) with Linux version 2.6.32-279.14.1.el6.x86 64 (mockbuild@x86- 002.build.bos.redhat.com) as their operating systems. ","4.6 Technical considerations 47 Table 4.2: Dataset description Dataset Train Set Test Set Characteristics #instances #instances dataset attribute 01-airline 129 15 Real Univariate 02-solar 361 41 Real Univariate 03-mauna 490 55 Real Univariate 04-wheat 333 37 Real Univariate 05-min-temp 899 101 Real Univariate 06-internet 909 91 Real Univariate 07-calls 162 18 Real Univariate 08-radio 216 24 Real Univariate 09-gas 428 48 Real Univariate 10-sulphuric 415 47 Real Univariate 11-unemploy 367 41 Real Univariate 12-quebec 893 107 Real Univariate 13-wages 661 74 Real Univariate 14-quebec-xl 4602 511 Real Multivariate (= 5) 15-concrete 927 103 Real Multivariate (= 9) 16-parkinsons 5287 588 Real Multivariate (= 21) 17-winewhite 4408 490 Real Multivariate (= 12) 4.6.2 Hardware considerations All experiments were performed using High Performance Computing (HPC) facilities provided by New Zealand eScience Infrastructure (NeSI). We performed Gaussian process model search on the nodes that supported Advanced Vector Extensions (AVX) instruction set i.e. NeSI nodes with Sandy Bridge or Ivy Bridge architecture 3 . Table 4.3 shows the AVX node specifications. 4.6.3 Source code availability The code changes that were performed as part of this dissertation at made available at https://bitbucket.org/manishka/abcd-research2 . This repository is trimmed down 3 http://www.eresearch.auckland.ac.nz/en/centre-for-eresearch/research-services/ computing-resources.html ","48 Stabilising extrapolation in an automatic data-analysis system Table 4.3: NeSI’s AVX Node specifications Architecture SandyBridge IvyBridge CPU Model Intel E5-2680 Intel E5-2697 Clock (GHz) 2.7 2.7 Cache (MB) 20 30 CPU Cores per node 16 24 Memory (GB) per node 128 128 or 256 version of original repository mentioned in Section 4.6.1 . Our repository contains code changes to support NeSI platform, GNU Octave scripting and also implements model scoring and model evaluation techniques discussed in Section 4.2 . 4.7 Summary Through this chapter, we described the challenges and criticism that are faced by ABCD — an existing automatic data-analysis system. We developed our hypotheses to tackle those problems. We described ABCD-SE’s proposed methods (with their algorithms) and set of experiments designed to test those hypotheses. Furthermore, we discussed the error measure, called Standardised Mean Squared Error (SMSE). This will be used to compare our proposed algorithms to existing system. We mentioned the range of real-world datasets, used to compare the various approaches. Finally, we described the technical platforms used to evaluate our proposed methods. In the next chapter, we examine the results of these experiments. ","5 Findings of experiments In this chapter, we present the results of experiments described in Section 4.3 , then we analyse those results and present our findings. We have structured this chapter as per the experiments described in Section 4.3 . Each section in this chapter is divided into two subsection, the first subsection shows the results and following subsection discusses the results. 5.1 Baseline experiments ABCD system runs parallel jobs to search for best covariance kernel composition. Gaussian process regression is performed using GPML toolbox. Lloyd [ 14 ] used Matlab to interface with GPML toolbox. We decided to use GNU Octave for our experiments, due to reason that Matlab (statistics package) licenses on NeSI were shared. The license sharing resulted in long waiting times and ABCD system became inefficient in running parallel model search jobs. For that reason, we performed these baseline experiments to assess the impact of using GNU Octave software for interfacing with GPML toolbox inside ABCD system. 5.1.1 Results We performed baseline experiments on four univariate datasets that were used by Lloyd [ 14 ]. We also kept same parameter settings as in original study. We ran these experiments 49 ","50 Findings of experiments once. Table 5.1 shows the negative log likelihood (NLL), BIC score and number of kernel hyper-parameters of the best model discovered after ten levels of search. Table 5.2 shows the result from original study [ 14 ]. These results are visualised in Figure 5.1 . Table 5.1: Baseline experiments results showing the best model using GNU Octave Dataset NLL BIC # parameters 01-airline 536.4491 1117.6264 9 06-internet 9427.7907 18993.7365 20 07-call-centre 772.1245 1611.7574 13 08-radio 226.6810 519.1297 12 Table 5.2: Results showing best model using Matlab (Lloyd [ 14 ]) Dataset NLL BIC # parameters 01-airline 536.4502 1117.6287 9 06-internet 9526.0615 19169.5548 17 07-call-centre 773.8376 1635.9554 17 08-radio 236.5201 522.3659 9 5.1.2 Analysis Model likelihoods and BIC scores as per Table 5.1 and Table 5.2 looks almost similar, there is some difference in number of kernel parameters learned for each dataset in both the cases. This is not concerning, as the final model may sometimes include an artefact of search procedure Lloyd [ 14 ]. Scatter plots, Figure 5.1 (a) and Figure 5.1 (b), show a clear correlation between BIC and Likelihood, this is due to the reason, BIC score is derived from likelihood estimate of Gaussian process model. Overall, these results suggest that there maybe no impact on using GNU Octave instead of Matlab to interface with GPML toolbox inside ABCD system. These results also confirm that the system has been properly configured on NeSI platform. On the basis of, results of these baseline experiments, further experimentation was performed. 5.2 Experiments with Hybrid SoD method We performed these experiments to evaluate, the amount of variation in the prediction accuracy, when model is discovered using, Hybrid SoD model evaluation method with a ","5.2 Experiments with Hybrid SoD method 51 model search strategy other than greedy and model scoring criterion different from BIC. In our experiments we chose, a wider search strategy like top-k (see Section 4.2.1 ) and model scoring criterion that considers size of training data such as AIC (see Equation 4.1 ). Combinations of model search strategy (greedy and top-k) and model scoring criterion (BIC and AIC), resulted in four set of experiments (see Table 4.1 ). This is often referred as “balanced design” experiments. We analyse the results together of both the experiments — Hybrid SoD (this section) and CV SoD (Section 5.3 ), in Section 5.4 . 5.2.1 Results All four set of experiments used the same system parameters described in Section 4.3.2 . In these experiments the model was searched for three levels in depth on 13 univariate and four multivariate real-world datasets. We repeated each set of experiments five times, every time with a different random number generator seed. We trained the model on 90% of the data and used remaining 10% to perform predictions. In every experiment, for each dataset, we computed mean-squared-errors (MSE) as per Equation 4.2 then we standardised MSE as per Equation 4.3 . SMSE allows us to compare prediction error of all datasets for each experiment. As experiments were repeated, we had five SMSE values for each dataset on every method, we averaged them to see their variability. Table 5.3 is reporting average of SMSE ± one unit standard deviation, and is visualised in Figure 5.3 the error bars represent the unit standard deviation. On some datasets all the methods performed equally well, in fact these methods had a very low SMSE prediction error 1 , for example, 02-solar, 16-parkinsons. We attribute this behaviour to very low variance in the dataset itself and inclusion of White Noise ( WN ) kernel in base kernels (see Appendix A , Section 3.4). Table 5.3: Results of experiments with Hybrid SoD model evaluation method Dataset Greedy AIC Top-k AIC Greedy BIC Top-k BIC 01-airline 0.0034 ± 0.0008 0.0061 ± 0.0029 0.0164 ± 0.0261 0.0077 ± 0.0112 02-solar 0.0000 ± 0.0000 0.0000 ± 0.0000 0.0000 ± 0.0000 0.0000 ± 0.0000 03-mauna 0.0059 ± 0.0070 0.0036 ± 0.0003 0.1777 ± 0.3083 0.0039 ± 0.0036 Continued on next page 1 We have rounded SMSE and standard deviations up to four decimal places, due to this some values appear as 0 . 000 (in reality they are very small values close to zero), for example standard deviations for datasets 14-quebec-xl, 15-concrete, 16-parkinsons for Top-k AIC or BIC experiments in Table 5.3 . ","52 Findings of experiments Table 5.3 – Continued from previous page Dataset Greedy AIC Top-k AIC Greedy BIC Top-k BIC 04-wheat 0.1754 ± 0.1419 0.3142 ± 0.0424 0.1997 ± 0.1460 0.3130 ± 0.0135 05-min-temp 0.0587 ± 0.0203 0.0697 ± 0.0275 0.0704 ± 0.0225 0.0491 ± 0.0009 06-internet 0.2690 ± 0.2161 0.1419 ± 0.1522 0.3852 ± 0.4082 0.0683 ± 0.0428 07-calls 1.4179 ± 0.9368 1.2571 ± 0.3906 0.6257 ± 1.0106 1.8601 ± 0.1097 08-radio 0.0435 ± 0.0462 0.0656 ± 0.0009 0.0551 ± 0.0592 0.0590 ± 0.0129 09-gas 0.4508 ± 0.3091 0.4844 ± 0.0837 1.1540 ± 1.7102 0.3197 ± 0.2253 10-sulphuric 0.3061 ± 0.1025 0.4242 ± 0.0794 0.2544 ± 0.0998 0.3430 ± 0.1193 11-unemploy 0.2478 ± 0.1901 0.1274 ± 0.1517 0.0970 ± 0.0705 0.0284 ± 0.0253 12-quebec 0.0238 ± 0.0142 0.2624 ± 0.3533 0.0206 ± 0.0103 0.0184 ± 0.0072 13-wages 0.4023 ± 0.0142 0.3634 ± 0.0533 0.3840 ± 0.0309 0.3689 ± 0.0250 14-quebec-xl 0.0068 ± 0.0002 0.0071 ± 0.0000 0.0069 ± 0.0002 0.0065 ± 0.0003 15-concrete 0.0182 ± 0.0102 0.0106 ± 0.0000 0.0130 ± 0.0044 0.0107 ± 0.0001 16-parkinsons 0.0000 ± 0.0000 0.0000 ± 0.0000 0.0000 ± 0.0000 0.0000 ± 0.0000 17-winewhite 0.0157 ± 0.0000 0.0157 ± 0.0000 0.0157 ± 0.0001 0.0156 ± 0.0000 5.3 Experiments with CV SoD method We performed these experiments to evaluate, the amount of variation in the prediction accuracy, when model is discovered using, a novel CV SoD model evaluation method as described in Section 4.2.2 . We used “balanced design” experimentation approach i.e. combinations of model search strategy (greedy and top-k) and model scoring criterion (BIC and AIC), resulted in four set of experiments (see Table 4.1 ). We collectively analyse the results of CV SoD method (this section) and Hybrid SoD method (Section 5.2 ) experiments in Section 5.4 . ","5.3 Experiments with CV SoD method 53 0 5000 10000 15000 20000 01-airline 06-internet 07-call-centre 08-radio Dataset Bayesian Information Criterion (BIC) Platform MATLAB OCTAVE Baseline experiments (a) BIC score comparison 0 2500 5000 7500 01-airline 06-internet 07-call-centre 08-radio Dataset Negative Log Likelihood (NLL) Platform MATLAB OCTAVE Baseline experiments (b) Likelihood comparison 9 12 15 18 01-airline 06-internet 07-call-centre 08-radio Dataset No. of kernel parameters Platform MATLAB OCTAVE Baseline experiments (c) Kernel parameters comparison Figure 5.1: Scatter plots of baseline experiments 5.3.1 Results All four set of experiments used the same system parameters described in Section 4.3.3 . We repeated each set of experiments five times, every time with a different random number generator seed. Model was searched for three levels in depth on 13 univariate and four multivariate real-world datasets. Table 5.4 is reporting average of SMSE ± one unit ","54 Findings of experiments standard deviation, for each set of experiment on each dataset and is visualised in Figure 5.4 the error bars represent the unit standard deviation. We see the similar trend that was seen in experiments with Hybrid SoD method, datasets like 02-solar and 16-parkinsons have very small SMSE prediction error 2 across all methods. We associate these trends, to the low variance in datasets and inclusion of White Noise ( WN ) kernel in base kernels (see Appendix A Section 3.4). Table 5.4: Results of experiments with CV SoD model evaluation method Dataset Greedy AIC Top-k AIC Greedy BIC Top-k BIC 01-airline 0.0036 ± 0.0024 0.0017 ± 0.0010 0.0051 ± 0.0041 0.0037 ± 0.0032 02-solar 0.0000 ± 0.0000 0.0000 ± 0.0000 0.0000 ± 0.0000 0.0000 ± 0.0000 03-mauna 0.0050 ± 0.0061 0.0111 ± 0.0160 0.0022 ± 0.0009 0.0013 ± 0.0013 04-wheat 0.2866 ± 0.0343 0.3192 ± 0.0061 0.2471 ± 0.1368 0.3192 ± 0.0051 05-min-temp 0.0791 ± 0.0204 0.0619 ± 0.0232 0.0493 ± 0.0006 0.0489 ± 0.0010 06-internet 0.3458 ± 0.3135 0.0513 ± 0.0161 0.2385 ± 0.3105 0.3012 ± 0.2729 07-calls 2.6567 ± 1.0172 1.7023 ± 0.3872 0.7947 ± 0.7670 2.0362 ± 0.3961 08-radio 0.0979 ± 0.1116 0.0971 ± 0.0180 0.0612 ± 0.0147 0.0708 ± 0.0184 09-gas 0.4386 ± 0.3006 0.5125 ± 0.0768 0.7894 ± 0.8900 0.3915 ± 0.0626 10-sulphuric 0.4670 ± 0.1453 0.2853 ± 0.1525 0.2954 ± 0.1203 0.3551 ± 0.1506 11-unemploy 0.2654 ± 0.3631 0.1363 ± 0.1392 0.2414 ± 0.2484 0.1591 ± 0.2319 12-quebec 0.0165 ± 0.0085 0.0138 ± 0.0029 0.0152 ± 0.0077 0.0192 ± 0.0114 13-wages 0.3793 ± 0.0498 0.4054 ± 0.0202 0.3226 ± 0.0454 0.3929 ± 0.0084 14-quebec-xl 0.0066 ± 0.0003 0.0063 ± 0.0000 0.0070 ± 0.0003 0.0071 ± 0.0000 15-concrete 0.0120 ± 0.0031 0.0167 ± 0.0000 0.0146 ± 0.0051 0.0108 ± 0.0000 16-parkinsons 0.0000 ± 0.0000 0.0000 ± 0.0000 0.0000 ± 0.0000 0.0000 ± 0.0000 17-winewhite 0.0157 ± 0.0000 0.0157 ± 0.0000 0.0156 ± 0.0000 0.0157 ± 0.0001 2 Values appearing as 0 . 000 in Table 5.4 are due to rounding of SMSE and standard deviations up to four decimal places. In fact they are very small values close to zero. ","5.4 Result analysis of experiments with Hybrid SoD and CV SoD methods 55 5.4 Result analysis of experiments with Hybrid SoD and CV SoD methods In this section, we discuss the results of balanced design experiments performed in Sections 5.2 and 5.3 . These experiments can also be referred as 2 × 2 × 2 factorial design, since we used two types of model evaluation methods such as Hybrid SoD and CV SoD, two kinds of search strategies, namely, greedy and top-k and two ways to score models i.e. BIC and AIC. We are validating three hypotheses, described in Section 4.1 , with the results of these experiments. Hypothesis 1 was developed to test whether composing a covariance kernel in a greedy search approach results in a better performing (or lower SMSE) Gaussian process model. Hypothesis 2 was formed to asses whether models penalised by BIC would be able to generalise better (or lower SMSE) on unseen data. Hypothesis 3 was created to check whether hybrid subset of data (SoD) approximation to Gaussian process regression provide stable extrapolation (or lower SMSE). 5.4.1 Analysis of variance (ANOVA) In order to validate these hypotheses, we employed analysis of variances (or ANOVA) method to compare model performance among different set of experiments. ANOVA allows us to check whether these prediction error or SMSE obtained by experiments differ from each other or not. Since error was standardised we considered SMSE of each dataset as a sample drawn from population using a particular method. In this way, we got 17 samples for each experiment (or method). We considered three factors (or independent variables) i.e. evaluation method, search approach and scoring criterion and SMSE as response (or dependent variable). Figure 5.2 shows the box plots for each of the three treatments (or factors). From eye-ball analysis, there does not seem to be much difference among them. First step is to check for interaction. An interaction among two factors indicates that different levels of one factor have different impacts (or effects) on different levels of other factor. Generally, parallel lines in interaction plots indicate no interaction among factors. Figure 5.5 suggests there could be interactions (lines are not parallel) among model scoring criterion and search approach and among model evaluation and search approach. With this informations we performed a three-way ANOVA with interactions between all the factors (see Listing 5.1 ). ","56 Findings of experiments AIC BIC 0.0 0.5 1.0 1.5 2.0 2.5 criterion SMSE greedy top-k 0.0 0.5 1.0 1.5 2.0 2.5 search SMSE CV SoD Hybrid SoD 0.0 0.5 1.0 1.5 2.0 2.5 evaluation SMSE Figure 5.2: Box plots for each level of independent variable Three-way ANOVA Listing 5.1 shows a three-way ANOVA performed including interactions with all thee factors. It suggests that there is no strong evidence to reject the null-hypothesis that all group means are same i.e. no method (for example, greedy search using BIC scoring in Hybrid SoD evaluation method) provides better prediction performance over the other (for example, top-k search using AIC scoring in CV SoD evaluation method). It also suggests that there is not evidence about interactions between the factors. Listing 5.1: Three-way ANOVA with interactions &gt; anova(lm(SMSE ~ criterion * search * evaluation , data = fr)) Analysis of Variance Table Response: SMSE Df Sum Sq Mean Sq F value Pr(&gt;F) criterion 1 0.0172 0.017238 0.0992 0.7533 search 1 0.0007 0.000669 0.0039 0.9506 evaluation 1 0.0293 0.029342 0.1689 0.6818 criterion:search 1 0.0410 0.040982 0.2359 0.6280 criterion:evaluation 1 0.0150 0.015031 0.0865 0.7691 search:evaluation 1 0.0019 0.001917 0.0110 0.9165 criterion:search:evaluation 1 0.0491 0.049118 0.2827 0.5958 Residuals 128 22.2364 0.173722 ","5.4 Result analysis of experiments with Hybrid SoD and CV SoD methods 57 0 1 2 3 07-calls [162,1] 09-gas [428,1] 13-wages [661,1] 06-internet [909,1] 0.0 0.2 0.4 10-sulphuric [415,1] 04-wheat [333,1] 11-unemploy [367,1] 03-mauna [490,1] 0.0 0.2 0.4 0.6 05-min-temp [899,1] 12-quebec [893,1] 08-radio [216,1] 17-winewhite [4408,12] 01-airline [129,1] 0.00 0.01 0.02 15-concrete [927,9] 14-quebec-xl [4602,5] 02-solar [361,1] 16-parkinsons [5287,21] Search &amp; Criterion greedy AIC top-k AIC greedy BIC top-k BIC Prediction accuracy of experiments with Hybrid SoD Datasets MSME Figure 5.3: Prediction error (SMSE) using Hybrid SoD model evaluation method ","58 Findings of experiments 0 1 2 3 07-calls [162,1] 09-gas [428,1] 13-wages [661,1] 06-internet [909,1] 0.0 0.2 0.4 0.6 10-sulphuric [415,1] 04-wheat [333,1] 11-unemploy [367,1] 03-mauna [490,1] 0.00 0.05 0.10 0.15 0.20 08-radio [216,1] 05-min-temp [899,1] 12-quebec [893,1] 17-winewhite [4408,12] 01-airline [129,1] 0.000 0.005 0.010 0.015 0.020 15-concrete [927,9] 14-quebec-xl [4602,5] 02-solar [361,1] 16-parkinsons [5287,21] Search &amp; Criterion greedy AIC top-k AIC greedy BIC top-k BIC Prediction accuracy of experiments with CV SoD Datasets MSME Figure 5.4: Prediction error (SMSE) using CV SoD model evaluation method ","5.4 Result analysis of experiments with Hybrid SoD and CV SoD methods 59 0.20 0.22 0.24 criterion mean of SMSE AIC BIC search top-k greedy 0.20 0.22 0.24 search mean of SMSE greedy top-k criterion BIC AIC 0.21 0.22 0.23 0.24 0.25 criterion mean of SMSE AIC BIC evaluation CV SoD Hybrid SoD 0.21 0.22 0.23 0.24 0.25 evaluation mean of SMSE CV SoD Hybrid SoD criterion AIC BIC 0.21 0.22 0.23 0.24 search mean of SMSE greedy top-k evaluation CV SoD Hybrid SoD 0.21 0.22 0.23 0.24 evaluation mean of SMSE CV SoD Hybrid SoD search top-k greedy Figure 5.5: Interaction plots of search versus criterion versus evaluation factors Moreover, traditional ANOVA has strong assumptions, such as the response should be normally distributed, there should be equal variances among the groups and groups should be independent. Violation of these assumptions could also reflect in non-significant p - values . ","60 Findings of experiments Based on this analysis, we can consider each experiment as an individual factor and repeated the analysis as single factor with multiple (or repeated) measures. Multivariate ANOVA Since, the groups (or samples) are same for each experiment, we employed a multivariate approach to one-way repeated measures analysis of variance (see Listing 5.2 ). Even multivariate ANOVA, did not provide enough evidence to reject the null-hypotheses ( p - value (0 . 311) &gt; 0 . 05). Listing 5.2: Multivariate ANOVA for repeated measures &gt; anova(result) Analysis of Variance Table Df Pillai approx F num Df den Df Pr(&gt;F) (Intercept) 1 0.49129 1.3796 7 10 0.311 Residuals 16 &gt; anova(result , test = Wilks ) Analysis of Variance Table Df Wilks approx F num Df den Df Pr(&gt;F) (Intercept) 1 0.50871 1.3796 7 10 0.311 Residuals 16 We then, analysed the trends in Figures 5.4 and 5.3 , these figures suggest that the standard deviations of SMSE prediction error is comparatively smaller for most of the datasets using CV SoD method. Also, the report generated by ABCD-SE system (see Appendix A Section 3.4) suggested that noise kernel present in composite kernel explains noise (or variability) of model. These reasons motivated us to perform another round of experiments without including White Noise kernel in the base kernels. 5.5 Experiments with no White Noise base kernel The purpose of these experiments was to check whether the high performance achieved in “balanced design” experiments was highly influenced by the presence of White Noise kernel in the base kernels and eventually in discovered structure. Automatic reports generated by ABCD-SE system (such as Appendix A ) describe the learnt model, it shows that most of the variability of explained by four base kernel — SE , Per , Lin and C and the remaining is explained by WN base kernel. ","5.5 Experiments with no White Noise base kernel 61 Thus, we decided to perform these experiments without WN kernel. “Balanced design” experiments consumed allocated resources on NeSI (CPU core hours = 100,000 hours). We got our allocation extended to perform these last experiments, however we could not perform these in a factorial design, due to resourcing restrictions. We kept same system parameters as in “balance design” experiments, with following exceptions kept search strategy as greedy and model scoring by BIC. We performed these experiments once on seven datasets. Two experiments were executed one with CV SoD model evaluation and other with Hybrid SoD model evaluation method. 5.5.1 Results Table 5.5 presents the prediction error SMSE 3 of learnt models in both experiments. We can see for dataset 02-solar the SMSE is seen as 0 . 000 due to rounding, but actual values of SMSE in CV SoD experiment was 7 . 8427 × 10 - 8 and in Hybrid SoD experiment was 8 . 4899 × 10 - 7 . Figure 5.6 shows the scatter plot of the results. ABCD-SE system generated the report for one of the dataset of this experiment (see Appendix B ). Table 5.5: Prediction error SMSE of the best models discovered in experiments with four base kernels, greedy search and BIC scoring Dataset CV SoD Hybrid SoD 01-airline 0.0157 0.6369 02-solar 0.0000 0.0000 04-wheat 0.2844 0.9800 05-min-temp 0.0493 0.0906 08-radio 0.0460 0.4180 10-sulphuric 0.4849 0.9178 13-wages 0.2624 0.2794 5.5.2 Analysis We start our analysis of these results by visualising them through a box-plot (see Figure 5.7 ). The box plot suggests that the means could be different i.e. the models evaluated using CV SoD could be performing better (lower SMSE) compared to Hybrid SoD. To 3 Rounded up to four decimal places. ","62 Findings of experiments 0.00 0.25 0.50 0.75 1.00 01-airline 02-solar 04-wheat 05-min-temp 08-radio 10-sulphuric 13-wages Dataset SMSE Model Evaluation CV SoD Hybrid SoD Model Evaluation without White Noise (WN) base kernel Figure 5.6: Prediction error (SMSE) of models constructed using four base kernels — SE , Per , Lin and C , in a greedy search strategy and selected based on highest BIC model score check this, we performed a paired t-test and confirmed it with traditional one-way ANOVA (see Listing 5.3 ). CV SoD Hybrid SoD 0.0 0.2 0.4 0.6 0.8 1.0 evaluation SMSE Figure 5.7: Box plot of each model evaluation method The paired t-test with p - value (0 . 016) &lt; 0 . 05 and ANOVA with p - value (0 . 031) &lt; 0 . 05 (in Listing 5.3 ) provide strong evidence to reject null-hypotheses — that both sample means are same. In paired t-test, we tested alternate hypotheses as H a : μ HybridSoD - μ CV SoD &gt; 0, as per strong evidence we can say that models evaluated with Hybrid SoD compared to CV SoD would have higher SMSE i.e. low performance. On average models evaluated with Hybrid SoD would have 0.311 more SMSE that those evaluated with CV SoD. ","5.6 Discussion 63 Listing 5.3: One-way ANOVA and Paired t-test ## Paired t-test data: as.numeric(com[, 3]) and as.numeric(com[, 2]) t = 2.8026 , df = 6, p-value = 0.01553 alternative hypothesis: true difference in means is greater than 0 95 percent confidence interval: 0.09550173 Inf sample estimates: mean of the differences 0.311445 ## ANOVA Error: dataset Df Sum Sq Mean Sq F value Pr(&gt;F) Residuals 6 0.836 0.1393 Error: dataset:evaluation Df Sum Sq Mean Sq F value Pr(&gt;F) evaluation 1 0.3395 0.3395 7.854 0.0311 * Residuals 6 0.2593 0.0432 --- Signif. codes: 0 *** ' 0.001 ** ' 0.01 * ' 0.05 . ' 0.1 ' 1 5.6 Discussion Through this dissertation, we were able to experiment several key areas of ABCD system like model search, scoring and evaluation. Baseline experiments provided a go-ahead with GNU Octave as the results were very similar. We then experimented in a factorial design way to validate three hypotheses. Experiments with Hybrid SoD and CV SoD methods could not provide enough statistical evidence to reject Hypothesis 1 , 2 and 3 . ANOVA analysis suggested that the group means are not different for different methods. By analysing generated reports and error bar plots we design a new set of experiment that excluded WN kernel from base kernels. These experiments provided strong statistical evidence to reject null-hypotheses that group means are same for model evaluated with Hybrid SoD and CV SoD. With paired t-test it was confirmed that models evaluated with Hybrid SoD, would on average have 0.311 higher SMSE (low predictive accuracy) than models evaluated with CV SoD method. We need further experimentation to support this claim that CV SoD provides stable extrapolation. Perhaps a factorial design experiment would be needed to confirm this, with at least five repetitions. ","64 Findings of experiments Table 5.6: Average number of models searched for each dataset, in all of the experiments, that searched three levels deep Dataset Greedy Top-k % increase 01-airline 1231 3811 209.66 02-solar 1272 3825 200.71 03-mauna 1238 3576 188.82 04-wheat 1107 3278 196.14 05-min-temp 1238 3574 188.61 06-internet 1215 3669 202.04 07-calls 1220 3804 211.68 08-radio 1224 3804 210.78 09-gas 1227 3705 202.04 10-sulphuric 1231 3849 212.74 11-unemploy 1170 3805 225.23 12-quebec 1245 3772 202.97 13-wages 1244 3536 184.23 14-quebec-xl 4238 12066 184.71 15-concrete 7189 22650 215.05 16-parkinsons 15973 39020 144.28 17-winewhite 9187 29184 217.67 We analysed the number of models searched for each dataset in every experiment. Since each experiment was repeated five times, we took the average number of models or covariance structures search using greedy approach and through top-k search and tabulated them data in Table 5.6 . On average, we saw the increase of ≈ 200% in number of models searched between the two methods. Top-k searches a much wider model space. The number of models searched are linearly proportional to number of dimensions in (or features of) input x . We can observe it when we compare univariate versus multivariate datasets. For that reason, top-k approach can potentially discover richer structures. In ","5.6 Discussion 65 order to truly see the effectiveness of top-k search procedure, we believe, we need to perform experiments without WN kernel. With respect to model criterion, changing it to AIC may not result in much difference in model performance as both AIC and BIC are derived from likelihood. The likelihood is dependent upon number of kernel parameters. Thus, in our opinion rather than penalising a model for high number of kernel parameters, if we try to optimise or select fewer parameters during model building that would result in much better extrapolation. In next section, we reflect back to our contributions, discuss limitations of our ap- proaches and briefly describe future research areas. ","66 Findings of experiments ","6 Conclusion In this section, we relate our results back to the contributions that were stated in the beginning of this dissertation. We examine the evidence obtained from the results of our experiments to show how each of our contributions are supported. We also draw out attention to any of our aims that were not supported by our experiments and limitations within our techniques. We discuss future work required to further validate the claims about providing stable extrapolation with high interpretability, made by CV SoD evaluation method. Finally, we reflect on the work we have done, considering the context of its subject area. We explore why and how our work is an important contribution to a rapidly developing, important and incredibly useful field. 6.1 Achievements We laid out three objectives in the beginning of this dissertation, they were tied together to achieve our main goal of improving prediction performance of an automatic data-analysis system known as Automatic Bayesian Covariance Discovery (ABCD) system. The following list highlights the major achievements of this work: • Wider search strategy : We implemented a much wider search strategy called top-k method. Models searched through top-k ( k = 3) strategy explored ≈ 200% 67 ","68 Conclusion more model space than greedy approach. This strategy has potential to exploit high performance parallel processing that modern hardware platforms provide. • Model selection based on information loss : We studied the effect of scoring or penalising the models using Akaike information criterion (AIC) by estabilishing the philosphy that each model represents certain information about the data and the model that has lowest information loss (or low AIC score) was selected as best. • Novel model approximation technique : We developed CV SoD method — a new approximation technique to evaluate Gaussian process regression models. This technique combined the simplicity of “subset of data” (SoD) method with stability of “cross-validation” (CV). Experiments with four base kernels strongly suggest that CV SoD method is indeed better at extrapolation than Hybrid SoD method. 6.2 Limitations In this section, we discuss the limitations in our proposed methods and the objectives which we could not achieve through this work. Our proposed approximation method of CV-SoD showed a little stabilisation over Hybrid SoD with models containing WN kernel or where there isn’t much variability in given data. However, most of the real world data has high variability and if the aim is to provide better extrapolation, we really should not be including White Noise ( WN ) in the base kernels. As including WN kernel may model “noise in data” as signal. Our proposed method of using top-k search, often times result in selection of models that are very similar to each other but they all have high scores. This could prevent the algorithm from finding richer more diverse structures. It is very similar to problem of convergence in genetic algorithms. On the other hand, we thought AIC would provide better predictive accuracy as it penalises (or scores) models by taking into the account of size of training data. However, based on our experimentation and result analysis we conclude that in order to improve the model selection we should be looking at ways to reduce or approximate kernel parameters rather that just depending upon information criterion. 6.3 Future Directions This section provides future research opportunities using ABCD system to further improve and enhance its abilities as a data science AI. We discuss these into three areas - model search, model evaluation and model selection. We draw inferences from our developed methods, experimentation and their findings. ","6.4 Reflections 69 Model search Our proposed method of searching model space in top-k fashion often results in models that are very similar to each other in their structure. This does not allow the system to discover newer and richer structures and could not perform better than greedy search. Just exploring wider space is not enough, we need to keep the models in top-k pool that are also diverse by means of an entropy or novelty measure. It would be interesting to evaluate distinct top-k search methods or random walk approaches. Furthermore, experimentation with four base kernels excluding noise kernel on current implementation of top-k method is required to provide statistical evidence to reject the hypothesis that greedy search discovers best composition. Model evaluation Our proposed method of combining careful selection of subset of data for evaluating Gaussian process model needs further testing to validate the claim that CV SoD provides better and more accurate predictions. It would be interesting to assess the improvement in prediction accuracy by performing model evaluation through other approximation techniques like Fully Independent Training Conditional (FITC), Improved Fast Gauss Transform (IFGT) or other Markov chain Monte Carlo (MCMC) methods. Model selection As discussed in Chapter 4 , model selection is one of the most explored research areas of machine learning. Further work could be done in this area using ABCD system by proposing a better multimodel inference technique that is based on entropy that weighs dissimilarity more. 6.4 Reflections As our reliance on digital systems increase for our routine and mundane tasks, the amount of data these systems, and our interactions with them, generate is reaching new heights every day. To discover meaningful patterns from large sets of data we require systems that can analyse and explain the data autonomously. ABCD is one such system that is trying to make sense out of the data, in a meaningful way. With cheap high computing power available at our dispense, we should be designing our data analysis systems in such a way, that they can explore large hypotheses space in parallel to make decisions like model selection. ","70 Conclusion Through the course of this work, we feel confident that such automatic data analysis systems can become better at learning and predicting the given data, by means of probabilistic non-parametric modelling. We therefore, think that our contributions would be deemed important as they are based on a deep understanding of how ABCD and Gaussian processes work. ","Appendices 71 ","","A Auto-generated report for 01-airline dataset (five base kernels) The following pages include an automatically generated report produced by the ABCD system. The search procedure described in Section 3.2.4 , used five base kernels — SE , Per , Lin , C and WN , to perform covariance kernel discovery. 73 ","74 Auto-generated report for 01-airline dataset (five base kernels) An automatic report for the dataset : 01-airline with White Noise (WN) base kernel The Automatic Statistician Abstract This report was produced by the Automatic Bayesian Covariance Discovery (ABCD) algorithm. 1 Executive summary The raw data and full model posterior with extrapolations are shown in figure 1 . Raw data 100 200 300 400 500 600 1950 1952 1954 1956 1958 1960 Full model posterior with extrapolations 0 100 200 300 400 500 600 700 1950 1952 1954 1956 1958 1960 Figure 1: Raw data (left) and model posterior with extrapolation (right) The structure search algorithm has identified five additive components in the data. The first 2 additive components explain 99.2% of the variation in the data as shown by the coefficient of determination ( R 2 ) values in table 1 . After the first 3 components the cross validated mean absolute error (MAE) does not decrease by more than 0.1%. This suggests that subsequent terms are modelling very short term trends, uncorrelated noise or are artefacts of the model or search procedure. Short summaries of the additive components are as follows: • A very smooth monotonically increasing function. • An approximately periodic function with a period of 1.0 years. • A smooth monotonically increasing function. • Uncorrelated noise. • A very approximately periodic function with a period of 1.0 years. Model checking statistics are summarised in table 2 in section 4 . These statistics have revealed highly statistically significant discrepancies between the data and model in component 2. The rest of the document is structured as follows. In section 2 the forms of the additive components are described and their posterior distributions are displayed. In section 3 the modelling assumptions of each component are discussed with reference to how this affects the extrapolations made by the model. Section 4 discusses model checking statistics, with plots showing the form of any detected discrepancies between the model and observed data. ","75 # R 2 (%) ∆ R 2 (%) Residual R 2 (%) Cross validated MAE Reduction in MAE (%) - - - - 259.50 - 1 85.3 85.3 85.3 30.67 88.2 2 99.2 14.0 94.8 9.82 68.0 3 99.8 0.5 69.3 7.73 21.3 4 100.0 0.2 99.9 7.73 0.0 5 100.0 0.0 100.0 7.74 -0.2 Table 1: Summary statistics for cumulative additive fits to the data. The residual coefficient of determination ( R 2 ) values are computed using the residuals from the previous fit as the target values; this measures how much of the residual variance is explained by each new component. The mean absolute error (MAE) is calculated using 10 fold cross validation with a contiguous block design; this measures the ability of the model to interpolate and extrapolate over moderate distances. The model is fit using the full data and the MAE values are calculated using this model; this double use of data means that the MAE values cannot be used reliably as an estimate of out-of-sample predictive performance. 2 Detailed discussion of additive components 2.1 Component 1 : A very smooth monotonically increasing function This component is a very smooth and monotonically increasing function. This component explains 85.3% of the total variance. The addition of this component reduces the cross validated MAE by 88.2% from 259.5 to 30.7. Posterior of component 1 100 150 200 250 300 350 400 450 1950 1952 1954 1956 1958 Sum of components up to component 1 100 200 300 400 500 600 1950 1952 1954 1956 1958 Figure 2: Pointwise posterior of component 1 (left) and the posterior of the cumulative sum of components with data (right) Residuals after component 1 -150 -100 -50 0 50 100 150 200 1950 1952 1954 1956 1958 Figure 3: Pointwise posterior of residuals after adding component 1 2.2 Component 2 : An approximately periodic function with a period of 1.0 years This component is approximately periodic with a period of 1.0 years. Across periods the shape of this function varies very smoothly. The shape of this function within each period has a typical lengthscale of 2.2 months. This component explains 94.8% of the residual variance; this increases the total variance explained from 85.3% to 99.2%. The addition of this component reduces the cross validated MAE by 67.98% from 30.67 to 9.82. ","76 Auto-generated report for 01-airline dataset (five base kernels) Posterior of component 2 -100 -50 0 50 100 150 1950 1952 1954 1956 1958 Sum of components up to component 2 0 100 200 300 400 500 600 1950 1952 1954 1956 1958 Figure 4: Pointwise posterior of component 2 (left) and the posterior of the cumulative sum of components with data (right) Residuals after component 2 -40 -30 -20 -10 0 10 20 30 1950 1952 1954 1956 1958 Figure 5: Pointwise posterior of residuals after adding component 2 2.3 Component 3 : A smooth monotonically increasing function This component is a smooth and monotonically increasing function with a typical lengthscale of 5.7 months. This component explains 69.3% of the residual variance; this increases the total variance explained from 99.2% to 99.8%. The addition of this component reduces the cross validated MAE by 21.33% from 9.82 to 7.73. Posterior of component 3 -30 -20 -10 0 10 20 30 1950 1952 1954 1956 1958 Sum of components up to component 3 0 100 200 300 400 500 600 1950 1952 1954 1956 1958 Figure 6: Pointwise posterior of component 3 (left) and the posterior of the cumulative sum of components with data (right) Residuals after component 3 -15 -10 -5 0 5 10 15 1950 1952 1954 1956 1958 Figure 7: Pointwise posterior of residuals after adding component 3 ","77 2.4 Component 4 : Uncorrelated noise This component models uncorrelated noise. This component explains 99.9% of the residual variance; this increases the total variance explained from 99.8% to 100.0%. The addition of this component reduces the cross validated MAE by 0.00% from 7.73 to 7.73. This component explains residual variance but does not improve MAE which suggests that this component describes very short term patterns, uncorrelated noise or is an artefact of the model or search procedure. Posterior of component 4 -15 -10 -5 0 5 10 15 1950 1952 1954 1956 1958 Sum of components up to component 4 0 100 200 300 400 500 600 1950 1952 1954 1956 1958 Figure 8: Pointwise posterior of component 4 (left) and the posterior of the cumulative sum of components with data (right) Residuals after component 4 -3 -2 -1 0 1 2 3 1950 1952 1954 1956 1958 Figure 9: Pointwise posterior of residuals after adding component 4 2.5 Component 5 : A very approximately periodic function with a period of 1.0 years This component is very approximately periodic with a period of 1.0 years. Across periods the shape of this function varies smoothly with a typical lengthscale of 5.7 months. Since this lengthscale is small relative to the period this component may more closely resemble a non-periodic smooth function. This component explains 100.0% of the residual variance; this increases the total variance explained from 100.0% to 100.0%. The addition of this component increases the cross validated MAE by 0.23% from 7.73 to 7.74. This component explains residual variance but does not improve MAE which suggests that this component describes very short term patterns, uncorrelated noise or is an artefact of the model or search procedure. Posterior of component 5 -3 -2 -1 0 1 2 3 1950 1952 1954 1956 1958 Sum of components up to component 5 0 100 200 300 400 500 600 1950 1952 1954 1956 1958 Figure 10: Pointwise posterior of component 5 (left) and the posterior of the cumulative sum of components with data (right) ","78 Auto-generated report for 01-airline dataset (five base kernels) 3 Extrapolation Summaries of the posterior distribution of the full model are shown in figure 11 . The plot on the left displays the mean of the posterior together with pointwise variance. The plot on the right displays three random samples from the posterior. Full model posterior with extrapolations 0 100 200 300 400 500 600 700 1950 1952 1954 1956 1958 1960 Random samples from the full model posterior 0 100 200 300 400 500 600 700 1950 1952 1954 1956 1958 1960 Figure 11: Full model posterior with extrapolation. Mean and pointwise variance (left) and three random samples (right) Below are descriptions of the modelling assumptions associated with each additive component and how they affect the predictive posterior. Plots of the pointwise posterior and samples from the posterior are also presented, showing extrapolations from each component and the cuulative sum of components. 3.1 Component 1 : A very smooth monotonically increasing function This component is assumed to continue very smoothly but is also assumed to be stationary so its distribution will eventually return to the prior. The prior distribution places mass on smooth func- tions with a marginal mean of zero and a typical lengthscale of 15.3 years. [This is a placeholder for a description of how quickly the posterior will start to resemble the prior]. Posterior of component 1 100 150 200 250 300 350 400 450 500 1950 1952 1954 1956 1958 1960 Random samples from the posterior of component 1 100 150 200 250 300 350 400 450 500 1950 1952 1954 1956 1958 1960 Sum of components up to component 1 100 200 300 400 500 600 1950 1952 1954 1956 1958 1960 Random samples from the cumulative posterior 100 150 200 250 300 350 400 450 500 1950 1952 1954 1956 1958 1960 Figure 12: Posterior of component 1 (top) and cumulative sum of components (bottom) with extrap- olation. Mean and pointwise variance (left) and three random samples from the posterior distribution (right). 3.2 Component 2 : An approximately periodic function with a period of 1.0 years This component is assumed to continue to be approximately periodic. The shape of the function is assumed to vary very smoothly between periods but will eventually return to the prior. The prior is entirely uncertain about the phase of the periodic function. Consequently the pointwise posterior will appear to lose its periodicity, but this merely reflects the uncertainty in the shape and phase ","79 of the function. [This is a placeholder for a description of how quickly the posterior will start to resemble the prior]. Posterior of component 2 -150 -100 -50 0 50 100 150 200 1950 1952 1954 1956 1958 1960 Random samples from the posterior of component 2 -150 -100 -50 0 50 100 150 200 1950 1952 1954 1956 1958 1960 Sum of components up to component 2 0 100 200 300 400 500 600 700 1950 1952 1954 1956 1958 1960 Random samples from the cumulative posterior 0 100 200 300 400 500 600 700 1950 1952 1954 1956 1958 1960 Figure 13: Posterior of component 2 (top) and cumulative sum of components (bottom) with extrap- olation. Mean and pointwise variance (left) and three random samples from the posterior distribution (right). 3.3 Component 3 : A smooth monotonically increasing function This component is assumed to continue smoothly but is also assumed to be stationary so its distribu- tion will return to the prior. The prior distribution places mass on smooth functions with a marginal mean of zero and a typical lengthscale of 5.7 months. [This is a placeholder for a description of how quickly the posterior will start to resemble the prior]. Posterior of component 3 -30 -20 -10 0 10 20 30 1950 1952 1954 1956 1958 1960 Random samples from the posterior of component 3 -20 -15 -10 -5 0 5 10 15 20 1950 1952 1954 1956 1958 1960 Sum of components up to component 3 0 100 200 300 400 500 600 700 1950 1952 1954 1956 1958 1960 Random samples from the cumulative posterior 0 100 200 300 400 500 600 700 1950 1952 1954 1956 1958 1960 Figure 14: Posterior of component 3 (top) and cumulative sum of components (bottom) with extrap- olation. Mean and pointwise variance (left) and three random samples from the posterior distribution (right). 3.4 Component 4 : Uncorrelated noise This component assumes the uncorrelated noise will continue indefinitely. ","80 Auto-generated report for 01-airline dataset (five base kernels) Posterior of component 4 -15 -10 -5 0 5 10 15 1950 1952 1954 1956 1958 1960 Random samples from the posterior of component 4 -30 -20 -10 0 10 20 30 1950 1952 1954 1956 1958 1960 Sum of components up to component 4 0 100 200 300 400 500 600 700 1950 1952 1954 1956 1958 1960 Random samples from the cumulative posterior 0 100 200 300 400 500 600 700 1950 1952 1954 1956 1958 1960 Figure 15: Posterior of component 4 (top) and cumulative sum of components (bottom) with extrap- olation. Mean and pointwise variance (left) and three random samples from the posterior distribution (right). 3.5 Component 5 : A very approximately periodic function with a period of 1.0 years This component is assumed to continue to be approximately periodic. The shape of the function is assumed to vary smoothly between periods but will quickly return to the prior. The prior is entirely uncertain about the phase of the periodic function. Consequently the pointwise posterior will appear to lose its periodicity, but this merely reflects the uncertainty in the shape and phase of the function. [This is a placeholder for a description of how quickly the posterior will start to resemble the prior]. Posterior of component 5 -3 -2 -1 0 1 2 3 1950 1952 1954 1956 1958 1960 Random samples from the posterior of component 5 -4 -3 -2 -1 0 1 2 3 1950 1952 1954 1956 1958 1960 Sum of components up to component 5 0 100 200 300 400 500 600 700 1950 1952 1954 1956 1958 1960 Random samples from the cumulative posterior 0 100 200 300 400 500 600 700 1950 1952 1954 1956 1958 1960 Figure 16: Posterior of component 5 (top) and cumulative sum of components (bottom) with extrap- olation. Mean and pointwise variance (left) and three random samples from the posterior distribution (right). 4 Model checking Several posterior predictive checks have been performed to assess how well the model describes the observed data. These tests take the form of comparing statistics evaluated on samples from the prior and posterior distributions for each additive component. The statistics are derived from autocorrelation function (ACF) estimates, periodograms and quantile-quantile (qq) plots. ","81 Table 2 displays cumulative probability and p -value estimates for these quantities. Cumulative prob- abilities near 0/1 indicate that the test statistic was lower/higher under the posterior compared to the prior unexpectedly often i.e. they contain the same information as a p -value for a two-tailed test and they also express if the test statistic was higher or lower than expected. p -values near 0 indicate that the test statistic was larger in magnitude under the posterior compared to the prior unexpectedly often. ACF Periodogram QQ # min min loc max max loc max min 1 0.208 0.570 0.136 0.368 0.345 0.561 2 0.775 0.695 0.003 0.358 0.008 0.042 3 0.305 0.391 0.881 0.617 0.601 0.481 4 0.405 0.370 0.314 0.456 0.480 0.507 5 0.500 0.500 0.483 0.495 0.512 0.512 Table 2: Model checking statistics for each component. Cumulative probabilities for minimum of autocorrelation function (ACF) and its location. Cumulative probabilities for maximum of peri- odogram and its location. p -values for maximum and minimum deviations of QQ-plot from straight line. The nature of any observed discrepancies is now described and plotted and hypotheses are given for the patterns in the data that may not be captured by the model. 4.1 Highly statistically significant discrepancies 4.1.1 Component 2 : An approximately periodic function with a period of 1.0 years The following discrepancies between the prior and posterior distributions for this component have been detected. • The maximum value of the periodogram is unexpectedly low. This discrepancy has an estimated p -value of 0.006 . • The qq plot has an unexpectedly large positive deviation from equality ( x = y ). This discrepancy has an estimated p -value of 0.008 . • The qq plot has an unexpectedly large negative deviation from equality ( x = y ). This discrepancy has an estimated p -value of 0.042. The positive deviation in the qq-plot can indicate heavy positive tails if it occurs at the right of the plot or light negative tails if it occurs as the left. The negative deviation in the qq-plot can indicate light positive tails if it occurs at the right of the plot or heavy negative tails if it occurs as the left. ","82 Auto-generated report for 01-airline dataset (five base kernels) Corelation coeficient Lag ACF uncertainty plot for component 2 -1 -0.5 0 0.5 1 2 4 6 8 10 Power / frequency Normalised frequency Periodogram uncertainty plot for component 2 -40 -20 0 20 40 60 80 0 0.2 0.4 0.6 0.8 1 QQ uncertainty plot for component 2 -600 -400 -200 0 200 400 600 -400 -300 -200 -100 0 100 200 300 400 Figure 17: ACF (top left), periodogram (top right) and quantile-quantile (bottom left) uncertainty plots. The blue line and shading are the pointwise mean and 90% confidence interval of the plots under the prior distribution for component 2. The green line and green dashed lines are the corre- sponding quantities under the posterior. 4.2 Model checking plots for components without statistically significant discrepancies 4.2.1 Component 1 : A very smooth monotonically increasing function No discrepancies between the prior and posterior of this component have been detected Corelation coeficient Lag ACF uncertainty plot for component 1 -1 -0.5 0 0.5 1 2 4 6 8 10 Power / frequency Normalised frequency Periodogram uncertainty plot for component 1 0 10 20 30 40 50 60 70 80 0 0.2 0.4 0.6 0.8 1 QQ uncertainty plot for component 1 -2000 -1500 -1000 -500 0 500 1000 1500 2000 -2000 -1000 0 1000 2000 Figure 18: ACF (top left), periodogram (top right) and quantile-quantile (bottom left) uncertainty plots. The blue line and shading are the pointwise mean and 90% confidence interval of the plots under the prior distribution for component 1. The green line and green dashed lines are the corre- sponding quantities under the posterior. 4.2.2 Component 3 : A smooth monotonically increasing function No discrepancies between the prior and posterior of this component have been detected ","83 Corelation coeficient Lag ACF uncertainty plot for component 3 -1 -0.5 0 0.5 1 2 4 6 8 10 Power / frequency Normalised frequency Periodogram uncertainty plot for component 3 -40 -30 -20 -10 0 10 20 30 0 0.2 0.4 0.6 0.8 1 QQ uncertainty plot for component 3 -30 -20 -10 0 10 20 -15 -10 -5 0 5 10 15 Figure 19: ACF (top left), periodogram (top right) and quantile-quantile (bottom left) uncertainty plots. The blue line and shading are the pointwise mean and 90% confidence interval of the plots under the prior distribution for component 3. The green line and green dashed lines are the corre- sponding quantities under the posterior. 4.2.3 Component 4 : Uncorrelated noise No discrepancies between the prior and posterior of this component have been detected Corelation coeficient Lag ACF uncertainty plot for component 4 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 2 4 6 8 10 Power / frequency Normalised frequency Periodogram uncertainty plot for component 4 -20 -15 -10 -5 0 5 10 15 20 0 0.2 0.4 0.6 0.8 1 QQ uncertainty plot for component 4 -30 -20 -10 0 10 20 30 -15 -10 -5 0 5 10 15 Figure 20: ACF (top left), periodogram (top right) and quantile-quantile (bottom left) uncertainty plots. The blue line and shading are the pointwise mean and 90% confidence interval of the plots under the prior distribution for component 4. The green line and green dashed lines are the corre- sponding quantities under the posterior. 4.2.4 Component 5 : A very approximately periodic function with a period of 1.0 years No discrepancies between the prior and posterior of this component have been detected ","84 Auto-generated report for 01-airline dataset (five base kernels) Corelation coeficient Lag ACF uncertainty plot for component 5 -1 -0.5 0 0.5 1 2 4 6 8 10 Power / frequency Normalised frequency Periodogram uncertainty plot for component 5 -60 -50 -40 -30 -20 -10 0 10 0 0.2 0.4 0.6 0.8 1 QQ uncertainty plot for component 5 -4 -3 -2 -1 0 1 2 3 4 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 Figure 21: ACF (top left), periodogram (top right) and quantile-quantile (bottom left) uncertainty plots. The blue line and shading are the pointwise mean and 90% confidence interval of the plots under the prior distribution for component 5. The green line and green dashed lines are the corre- sponding quantities under the posterior. 5 MMD - experimental section # mmd 1 0.000 2 0.000 3 0.001 4 0.412 5 0.130 Table 3: MMD p -values 5.0.1 Component 1 : A very smooth monotonically increasing function 0 50 100 150 200 250 0 50 100 150 200 250 MMD two sample test plot for component 1 0 0.1 0.2 0.3 0.4 Figure 22: MMD plot ","85 5.0.2 Component 2 : An approximately periodic function with a period of 1.0 years 0 50 100 150 200 250 0 50 100 150 200 250 MMD two sample test plot for component 2 0 0.1 0.2 0.3 0.4 0.5 0.6 Figure 23: MMD plot 5.0.3 Component 3 : A smooth monotonically increasing function 0 50 100 150 200 250 0 50 100 150 200 250 MMD two sample test plot for component 3 -0.06 -0.04 -0.02 0 0.02 0.04 Figure 24: MMD plot 5.0.4 Component 4 : Uncorrelated noise 0 50 100 150 200 250 0 50 100 150 200 250 MMD two sample test plot for component 4 -0.03 -0.02 -0.01 0 0.01 0.02 0.03 0.04 0.05 Figure 25: MMD plot 5.0.5 Component 5 : A very approximately periodic function with a period of 1.0 years 0 50 100 150 200 250 0 50 100 150 200 250 MMD two sample test plot for component 5 -0.06 -0.04 -0.02 0 0.02 0.04 Figure 26: MMD plot ","86 Auto-generated report for 01-airline dataset (five base kernels) ","B Auto-generated report for 01-airline dataset (four base kernels) The following pages include an automatically generated report produced by the ABCD system. he search procedure described in Section 3.2.4 , used four base kernels — SE , Per , Lin and C , to perform covariance kernel discovery. 87 ","88 Auto-generated report for 01-airline dataset (four base kernels) An automatic report for the dataset : 01-airline without White Noise (WN) base kernel The Automatic Statistician Abstract This report was produced by the Automatic Bayesian Covariance Discovery (ABCD) algorithm. 1 Executive summary The raw data and full model posterior with extrapolations are shown in figure 1 . Raw data 100 200 300 400 500 600 1950 1952 1954 1956 1958 1960 Full model posterior with extrapolations -2000 -1500 -1000 -500 0 500 1000 1500 2000 1950 1952 1954 1956 1958 1960 Figure 1: Raw data (left) and model posterior with extrapolation (right) The structure search algorithm has identified two additive components in the data. The first additive component explains 94.2% of the variation in the data as shown by the coefficient of determination ( R 2 ) values in table 1 . The 2 additive components explain 100.0% of the variation in the data. After the first component the cross validated mean absolute error (MAE) does not decrease by more than 0.1%. This suggests that subsequent terms are modelling very short term trends, uncorrelated noise or are artefacts of the model or search procedure. Short summaries of the additive components are as follows: • A smooth monotonically increasing function. • A very approximately periodic function with a period of 1.0 years. Model checking statistics are summarised in table 2 in section 4 . These statistics have revealed highly statistically significant discrepancies between the data and model in component 1. The rest of the document is structured as follows. In section 2 the forms of the additive components are described and their posterior distributions are displayed. In section 3 the modelling assumptions of each component are discussed with reference to how this affects the extrapolations made by the model. Section 4 discusses model checking statistics, with plots showing the form of any detected discrepancies between the model and observed data. ","89 # R 2 (%) ∆ R 2 (%) Residual R 2 (%) Cross validated MAE Reduction in MAE (%) - - - - 259.50 - 1 94.2 94.2 94.2 127.04 51.0 2 100.0 5.8 100.0 131.42 -3.4 Table 1: Summary statistics for cumulative additive fits to the data. The residual coefficient of determination ( R 2 ) values are computed using the residuals from the previous fit as the target values; this measures how much of the residual variance is explained by each new component. The mean absolute error (MAE) is calculated using 10 fold cross validation with a contiguous block design; this measures the ability of the model to interpolate and extrapolate over moderate distances. The model is fit using the full data and the MAE values are calculated using this model; this double use of data means that the MAE values cannot be used reliably as an estimate of out-of-sample predictive performance. 2 Detailed discussion of additive components 2.1 Component 1 : A smooth monotonically increasing function This component is a smooth and monotonically increasing function with a typical lengthscale of 5.7 months. This component explains 94.2% of the total variance. The addition of this component reduces the cross validated MAE by 51.0% from 259.5 to 127.0. Posterior of component 1 -200 0 200 400 600 800 1950 1952 1954 1956 1958 Sum of components up to component 1 -200 0 200 400 600 800 1950 1952 1954 1956 1958 Figure 2: Pointwise posterior of component 1 (left) and the posterior of the cumulative sum of components with data (right) Residuals after component 1 -300 -200 -100 0 100 200 300 1950 1952 1954 1956 1958 Figure 3: Pointwise posterior of residuals after adding component 1 2.2 Component 2 : A very approximately periodic function with a period of 1.0 years This component is very approximately periodic with a period of 1.0 years. Across periods the shape of this function varies smoothly with a typical lengthscale of 5.7 months. Since this lengthscale is small relative to the period this component may more closely resemble a non-periodic smooth function. This component explains 100.0% of the residual variance; this increases the total variance explained from 94.2% to 100.0%. The addition of this component increases the cross validated MAE by 3.45% from 127.04 to 131.42. This component explains residual variance but does not improve MAE which ","90 Auto-generated report for 01-airline dataset (four base kernels) suggests that this component describes very short term patterns, uncorrelated noise or is an artefact of the model or search procedure. Posterior of component 2 -300 -200 -100 0 100 200 300 1950 1952 1954 1956 1958 Sum of components up to component 2 100 200 300 400 500 600 1950 1952 1954 1956 1958 Figure 4: Pointwise posterior of component 2 (left) and the posterior of the cumulative sum of components with data (right) 3 Extrapolation Summaries of the posterior distribution of the full model are shown in figure 5 . The plot on the left displays the mean of the posterior together with pointwise variance. The plot on the right displays three random samples from the posterior. Full model posterior with extrapolations -2000 -1500 -1000 -500 0 500 1000 1500 2000 1950 1952 1954 1956 1958 1960 Random samples from the full model posterior -1500 -1000 -500 0 500 1000 1500 2000 1950 1952 1954 1956 1958 1960 Figure 5: Full model posterior with extrapolation. Mean and pointwise variance (left) and three random samples (right) Below are descriptions of the modelling assumptions associated with each additive component and how they affect the predictive posterior. Plots of the pointwise posterior and samples from the posterior are also presented, showing extrapolations from each component and the cuulative sum of components. 3.1 Component 1 : A smooth monotonically increasing function This component is assumed to continue smoothly but is also assumed to be stationary so its distribu- tion will return to the prior. The prior distribution places mass on smooth functions with a marginal mean of zero and a typical lengthscale of 5.7 months. [This is a placeholder for a description of how quickly the posterior will start to resemble the prior]. ","91 Posterior of component 1 -2000 -1500 -1000 -500 0 500 1000 1500 2000 1950 1952 1954 1956 1958 1960 Random samples from the posterior of component 1 -500 0 500 1000 1500 1950 1952 1954 1956 1958 1960 Sum of components up to component 1 -2000 -1500 -1000 -500 0 500 1000 1500 2000 1950 1952 1954 1956 1958 1960 Random samples from the cumulative posterior -400 -200 0 200 400 600 800 1000 1950 1952 1954 1956 1958 1960 Figure 6: Posterior of component 1 (top) and cumulative sum of components (bottom) with extrapo- lation. Mean and pointwise variance (left) and three random samples from the posterior distribution (right). 3.2 Component 2 : A very approximately periodic function with a period of 1.0 years This component is assumed to continue to be approximately periodic. The shape of the function is assumed to vary smoothly between periods but will quickly return to the prior. The prior is entirely uncertain about the phase of the periodic function. Consequently the pointwise posterior will appear to lose its periodicity, but this merely reflects the uncertainty in the shape and phase of the function. [This is a placeholder for a description of how quickly the posterior will start to resemble the prior]. Posterior of component 2 -300 -200 -100 0 100 200 300 1950 1952 1954 1956 1958 1960 Random samples from the posterior of component 2 -400 -300 -200 -100 0 100 200 300 1950 1952 1954 1956 1958 1960 Sum of components up to component 2 -2000 -1500 -1000 -500 0 500 1000 1500 2000 1950 1952 1954 1956 1958 1960 Random samples from the cumulative posterior -1000 -500 0 500 1000 1500 1950 1952 1954 1956 1958 1960 Figure 7: Posterior of component 2 (top) and cumulative sum of components (bottom) with extrapo- lation. Mean and pointwise variance (left) and three random samples from the posterior distribution (right). 4 Model checking Several posterior predictive checks have been performed to assess how well the model describes the observed data. These tests take the form of comparing statistics evaluated on samples from the prior and posterior distributions for each additive component. The statistics are derived from autocorrelation function (ACF) estimates, periodograms and quantile-quantile (qq) plots. ","92 Auto-generated report for 01-airline dataset (four base kernels) Table 2 displays cumulative probability and p -value estimates for these quantities. Cumulative prob- abilities near 0/1 indicate that the test statistic was lower/higher under the posterior compared to the prior unexpectedly often i.e. they contain the same information as a p -value for a two-tailed test and they also express if the test statistic was higher or lower than expected. p -values near 0 indicate that the test statistic was larger in magnitude under the posterior compared to the prior unexpectedly often. ACF Periodogram QQ # min min loc max max loc max min 1 0.693 0.931 0.006 0.010 0.001 0.016 2 0.161 0.889 0.222 0.219 0.133 0.142 Table 2: Model checking statistics for each component. Cumulative probabilities for minimum of autocorrelation function (ACF) and its location. Cumulative probabilities for maximum of peri- odogram and its location. p -values for maximum and minimum deviations of QQ-plot from straight line. The nature of any observed discrepancies is now described and plotted and hypotheses are given for the patterns in the data that may not be captured by the model. 4.1 Highly statistically significant discrepancies 4.1.1 Component 1 : A smooth monotonically increasing function The following discrepancies between the prior and posterior distributions for this component have been detected. • The qq plot has an unexpectedly large positive deviation from equality ( x = y ). This discrepancy has an estimated p -value of 0.001 . • The maximum value of the periodogram is unexpectedly low. This discrepancy has an estimated p -value of 0.012. • The qq plot has an unexpectedly large negative deviation from equality ( x = y ). This discrepancy has an estimated p -value of 0.016. • The frequency of the maximum value of the periodogram is unexpectedly low. This dis- crepancy has an estimated p -value of 0.020. The positive deviation in the qq-plot can indicate heavy positive tails if it occurs at the right of the plot or light negative tails if it occurs as the left. The negative deviation in the qq-plot can indicate light positive tails if it occurs at the right of the plot or heavy negative tails if it occurs as the left. ","93 Corelation coeficient Lag ACF uncertainty plot for component 1 -1 -0.5 0 0.5 1 2 4 6 8 10 Power / frequency Normalised frequency Periodogram uncertainty plot for component 1 0 10 20 30 40 50 60 70 0 0.2 0.4 0.6 0.8 1 QQ uncertainty plot for component 1 -3000 -2000 -1000 0 1000 2000 3000 -1500 -1000 -500 0 500 1000 1500 Figure 8: ACF (top left), periodogram (top right) and quantile-quantile (bottom left) uncertainty plots. The blue line and shading are the pointwise mean and 90% confidence interval of the plots under the prior distribution for component 1. The green line and green dashed lines are the corre- sponding quantities under the posterior. 4.2 Model checking plots for components without statistically significant discrepancies 4.2.1 Component 2 : A very approximately periodic function with a period of 1.0 years No discrepancies between the prior and posterior of this component have been detected Corelation coeficient Lag ACF uncertainty plot for component 2 -1 -0.5 0 0.5 1 2 4 6 8 10 Power / frequency Normalised frequency Periodogram uncertainty plot for component 2 -20 -10 0 10 20 30 40 50 0 0.2 0.4 0.6 0.8 1 QQ uncertainty plot for component 2 -600 -400 -200 0 200 400 600 -300 -200 -100 0 100 200 300 Figure 9: ACF (top left), periodogram (top right) and quantile-quantile (bottom left) uncertainty plots. The blue line and shading are the pointwise mean and 90% confidence interval of the plots under the prior distribution for component 2. The green line and green dashed lines are the corre- sponding quantities under the posterior. ","94 Auto-generated report for 01-airline dataset (four base kernels) # mmd 1 0.000 2 0.000 Table 3: MMD p -values 5 MMD - experimental section 5.0.1 Component 1 : A smooth monotonically increasing function 0 50 100 150 200 250 0 50 100 150 200 250 MMD two sample test plot for component 1 -1.8e-07 -1.6e-07 -1.4e-07 -1.2e-07 -1e-07 -8e-08 -6e-08 -4e-08 -2e-08 0 Figure 10: MMD plot 5.0.2 Component 2 : A very approximately periodic function with a period of 1.0 years 0 50 100 150 200 250 0 50 100 150 200 250 MMD two sample test plot for component 2 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Figure 11: MMD plot ","References [1] Hirotogu Akaike. “Information Theory and an Extension of the Maximum Likelihood Principle”. en. In: Selected Papers of Hirotugu Akaike . Ed. by Emanuel Parzen, Kunio Tanabe, and Genshiro Kitagawa. Springer Series in Statistics. DOI: 10.1007/978-1-4612-1694-0 15. Springer New York, 1998, pp. 199–213. isbn : 978-1-4612-7248-9 978-1-4612-1694-0. url : http://link.springer.com/ chapter/10.1007/978-1-4612-1694-0_15 . [2] Kenneth. P. Burnham and David. R. Anderson. “Multimodel Inference: Understanding AIC and BIC in Model Selection”. en. In: Sociological Methods &amp; Research 33.2 (Nov. 2004), pp. 261–304. issn : 0049-1241. doi : 10.1177/0049124104268644 . url : http://smr.sagepub.com/cgi/doi/10. 1177/0049124104268644 . [3] Krzysztof Chalupka, Christopher KI Williams, and Iain Murray. “A framework for evaluating approximation methods for Gaussian process regression”. In: Journal of Machine Learning Research 14.Feb (2013), pp. 333–350. url : http://www.jmlr.org/papers/v14/chalupka13a.html . [4] David Duvenaud et al. “Structure Discovery in Nonparametric Regression through Compositional Kernel Search”. In: arXiv:1302.4922 [cs, stat] (Feb. 2013). arXiv: 1302.4922. url : http://arxiv. org/abs/1302.4922 . [5] Peter L Flom and David L Cassell. “Stopping stepwise: Why stepwise and similar selection methods are bad, and what you should use”. In: Baltimore, Maryland, Nov. 2007. url : www.lexjansen. com/pnwsug/2008/DavidCassell-StoppingStepwise.pdf . [6] Malcolm R Forster. “Key Concepts in Model Selection: Performance and Generalizability”. In: Jour- nal of Mathematical Psychology 44.1 (Mar. 2000), pp. 205–231. issn : 0022-2496. doi : 10.1006/jmps. 1999.1284 . url : http://www.sciencedirect.com/science/article/pii/S0022249699912841 . [7] Malcolm R Forster. “The new science of simplicity”. In: Simplicity, inference and modelling: keeping it sophisticatedly simple (2001), pp. 83–119. url : http://philosophy.wisc.edu/forster/ SciSimp.pdf . [8] Mehmet Gonen and Ethem Alpaydn. “Multiple Kernel Learning Algorithms”. In: Journal of Machine Learning Research 12.Jul (2011), pp. 2211–2268. issn : ISSN 1533-7928. url : http: //www.jmlr.org/papers/v12/gonen11a.html . [9] Roger Grosse et al. “Exploiting compositionality to explore a large space of model structures”. In: arXiv preprint arXiv:1210.4856 (2012). url : http://arxiv.org/abs/1210.4856 . [10] R. R. Hocking. “The Analysis and Selection of Variables in Linear Regression”. In: Biometrics 32.1 (1976), pp. 1–49. issn : 0006-341X. doi : 10.2307/2529336 . url : http://www.jstor.org/stable/ 2529336 . 95 ","96 REFERENCES [11] Tony Jebara. “Action-reaction learning: Analysis and synthesis of human behaviour”. PhD thesis. Citeseer, 1998. url : http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.30. 8702&amp;rep=rep1&amp;type=pdf . [12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. “ImageNet Classification with Deep Convolutional Neural Networks”. In: Advances in Neural Information Processing Systems 25 . Ed. by F. Pereira et al. Curran Associates, Inc., 2012, pp. 1097–1105. url : http://papers.nips.cc/ paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf . [13] Y. LeCun et al. “Backpropagation Applied to Handwritten Zip Code Recognition”. In: Neural Comput. 1.4 (Dec. 1989), pp. 541–551. issn : 0899-7667. doi : 10.1162/neco.1989.1.4.541 . url : http://dx.doi.org/10.1162/neco.1989.1.4.541 . [14] James Robert Lloyd. “Representation, learning, description and criticism of probabilistic models with applications to networks, functions and relational data”. en. PhD thesis. UK: University of Cambridge, 2015. url : https://github.com/jamesrobertlloyd/phd-thesis/raw/master/ thesis-final.pdf . [15] James Robert Lloyd et al. “Automatic construction and natural-language description of nonpara- metric regression models”. In: arXiv preprint arXiv:1402.4304 (2014). url : http://arxiv.org/ abs/1402.4304 . [16] William Mendenhall and Terry T. Sincich. A Second Course in Statistics: Regression Analysis . English. 7 edition. Boston: Pearson, Jan. 2011. isbn : 978-0-321-69169-9. [17] Kevin P. Murphy. Machine Learning: A Probabilistic Perspective . The MIT Press, 2012. isbn : 978-0-262-01802-9. [18] Radford M. Neal. Bayesian Learning for Neural Networks . en. Lecture Notes in Statistics 118. DOI: 10.1007/978-1-4612-0745-0. Springer New York, 1996. isbn : 978-0-387-94724-2 978-1-4612-0745-0. url : http://link.springer.com/book/10.1007/978-1-4612-0745-0 . [19] Peter Orbanz and Yee Whye Teh. “Bayesian Nonparametric Models”. en. In: Encyclopedia of Machine Learning . Ed. by Claude Sammut and Geoffrey I. Webb. DOI: 10.1007/978-0-387-30164- 8 66. Springer US, 2011, pp. 81–89. isbn : 978-0-387-30768-8 978-0-387-30164-8. url : http://link. springer.com/referenceworkentry/10.1007/978-0-387-30164-8_66 . [20] Tony A. Plate. “Accuracy versus interpretability in flexible modeling: implementing a tradeoff using Gaussian process models”. In: Behaviourmetrika Special Issue on interpreting Neural Network Models 26 (1999), pp. 29–50. [21] H. Poon and P. Domingos. “Sum-product networks: A new deep architecture”. In: 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops) . Nov. 2011, pp. 689– 690. doi : 10.1109/ICCVW.2011.6130310 . [22] Carl Edward Rasmussen and Zoubin Ghahramani. “Occam’s Razor”. In: Advances in Neural Information Processing Systems 13 . Ed. by T. K. Leen, T. G. Dietterich, and V. Tresp. MIT Press, 2001, pp. 294–300. url : http://papers.nips.cc/paper/1925-occams-razor.pdf . [23] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning . Adaptive computation and machine learning. OCLC: ocm61285753. Cambridge, Mass: MIT Press, 2006. isbn : 978-0-262-18253-9. [24] Carl Sagan. Cosmos . English. Paris: RCA, 1981. isbn : 978-2-7242-1094-1. ","ONLINE RESOURCES 97 [25] Gideon Schwarz. “Estimating the Dimension of a Model”. EN. In: The Annals of Statistics 6.2 (Mar. 1978), pp. 461–464. issn : 0090-5364, 2168-8966. doi : 10.1214/aos/1176344136 . url : http://projecteuclid.org/euclid.aos/1176344136 . [26] Nariaki Sugiura. “Further analysts of the data by akaike’ s information criterion and the finite corrections”. In: Communications in Statistics - Theory and Methods 7.1 (Jan. 1978), pp. 13– 26. issn : 0361-0926. doi : 10.1080/03610927808827599 . url : http://dx.doi.org/10.1080/ 03610927808827599 . [27] Andrew Gordon Wilson. “Covariance kernels for fast automatic pattern discovery and extrapolation with Gaussian processes”. PhD thesis. University of Cambridge, 2014. url : https://www.cs.cmu. edu/ ~ andrewgw/andrewgwthesis.pdf . [28] Andrew Gordon Wilson et al. “GPatt: Fast Multidimensional Pattern Extrapolation with Gaussian Processes”. In: arXiv:1310.5288 [cs, stat] (Oct. 2013). arXiv: 1310.5288. url : http://arxiv.org/ abs/1310.5288 . Online Resources [29] James Robert Lloyd. James Lloyd (University of Cambridge) - The Automatic Statistician . Harvard University, 2014. url : http://projects.iq.harvard.edu/applied.stats.workshop-gov3009/ presentations/automatic-statistician-presenter-james-lloyd . "]